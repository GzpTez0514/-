{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch深度学习06-实现神经网络的正向传播.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNqIvM/ENit0x+OaDwz87qT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GzpTez0514/-/blob/main/Pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A006_%E5%AE%9E%E7%8E%B0%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%AD%A3%E5%90%91%E4%BC%A0%E6%92%AD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0RGvOFgIjW-x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96d98afe-500c-4895-94e0-d6650a071cf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.return_types.max(\n",
            "values=tensor([0.4140, 0.4210, 0.4011, 0.4253, 0.4321, 0.4133, 0.4034, 0.4247, 0.4265,\n",
            "        0.4131, 0.4177, 0.4101, 0.4164, 0.4234, 0.4195, 0.4163, 0.4154, 0.4090,\n",
            "        0.4183, 0.4149, 0.4096, 0.4119, 0.4098, 0.4181, 0.4208, 0.4206, 0.4203,\n",
            "        0.4163, 0.4210, 0.4121, 0.4131, 0.4125, 0.4157, 0.4117, 0.4160, 0.4157,\n",
            "        0.4151, 0.4197, 0.4161, 0.4134, 0.4175, 0.4201, 0.4183, 0.4127, 0.4214,\n",
            "        0.4193, 0.4058, 0.4172, 0.4112, 0.4142, 0.4171, 0.4119, 0.4150, 0.4133,\n",
            "        0.4173, 0.4133, 0.4178, 0.4109, 0.4197, 0.4153, 0.4129, 0.4158, 0.4190,\n",
            "        0.4183, 0.4139, 0.4182, 0.4113, 0.4115, 0.4169, 0.4214, 0.4149, 0.4137,\n",
            "        0.4074, 0.4179, 0.4177, 0.4174, 0.4170, 0.4101, 0.4191, 0.4084, 0.4232,\n",
            "        0.4212, 0.4190, 0.4220, 0.4188, 0.4182, 0.4129, 0.4202, 0.4127, 0.4119,\n",
            "        0.4125, 0.4171, 0.4164, 0.4103, 0.4147, 0.4109, 0.4185, 0.4124, 0.4134,\n",
            "        0.4144, 0.4233, 0.4104, 0.4276, 0.4213, 0.4165, 0.4218, 0.4152, 0.4117,\n",
            "        0.4261, 0.4257, 0.4171, 0.4233, 0.4167, 0.4173, 0.4183, 0.4193, 0.4156,\n",
            "        0.4163, 0.4245, 0.4073, 0.4123, 0.4134, 0.4155, 0.4071, 0.4180, 0.4199,\n",
            "        0.4223, 0.4160, 0.4106, 0.4145, 0.4259, 0.4258, 0.4213, 0.4132, 0.4157,\n",
            "        0.4160, 0.4136, 0.4243, 0.4129, 0.4136, 0.4146, 0.4206, 0.4196, 0.4141,\n",
            "        0.4176, 0.4207, 0.4200, 0.4209, 0.4159, 0.4092, 0.4198, 0.4084, 0.4091,\n",
            "        0.4144, 0.4154, 0.4191, 0.4121, 0.4128, 0.4182, 0.4178, 0.4105, 0.4179,\n",
            "        0.4224, 0.4136, 0.4130, 0.4184, 0.4179, 0.4176, 0.4151, 0.4096, 0.4143,\n",
            "        0.4207, 0.4143, 0.4207, 0.4180, 0.4219, 0.4141, 0.4184, 0.4139, 0.4161,\n",
            "        0.4157, 0.4182, 0.4093, 0.4109, 0.4140, 0.4242, 0.4089, 0.4125, 0.4084,\n",
            "        0.4133, 0.4145, 0.4151, 0.4181, 0.4216, 0.4147, 0.4128, 0.4093, 0.4117,\n",
            "        0.4188, 0.4113, 0.4143, 0.4197, 0.4083, 0.4172, 0.4182, 0.4110, 0.4129,\n",
            "        0.4149, 0.4145, 0.4137, 0.4124, 0.4202, 0.4096, 0.4139, 0.4150, 0.4235,\n",
            "        0.4197, 0.4126, 0.4183, 0.4207, 0.4239, 0.4176, 0.4128, 0.4244, 0.4119,\n",
            "        0.4177, 0.4168, 0.4188, 0.4222, 0.4133, 0.4162, 0.4153, 0.4136, 0.4189,\n",
            "        0.4122, 0.4111, 0.4150, 0.4120, 0.4208, 0.4085, 0.4123, 0.4115, 0.4147,\n",
            "        0.4188, 0.4173, 0.4214, 0.4242, 0.4253, 0.4155, 0.4193, 0.4168, 0.4232,\n",
            "        0.4128, 0.4258, 0.4164, 0.4096, 0.4179, 0.4100, 0.4152, 0.4154, 0.4222,\n",
            "        0.4082, 0.4203, 0.4143, 0.4264, 0.4130, 0.4176, 0.4161, 0.4243, 0.4193,\n",
            "        0.4253, 0.4169, 0.4094, 0.4155, 0.4162, 0.4094, 0.4181, 0.4134, 0.4210,\n",
            "        0.4208, 0.4233, 0.4090, 0.4139, 0.4181, 0.4142, 0.4178, 0.4222, 0.4131,\n",
            "        0.4108, 0.4164, 0.4167, 0.4195, 0.4065, 0.4169, 0.4084, 0.4234, 0.4161,\n",
            "        0.4135, 0.4090, 0.4133, 0.4201, 0.4231, 0.4164, 0.4254, 0.4132, 0.4200,\n",
            "        0.4199, 0.4131, 0.4156, 0.4217, 0.4116, 0.4134, 0.4117, 0.4173, 0.4172,\n",
            "        0.4134, 0.4221, 0.4132, 0.4208, 0.4195, 0.4202, 0.4153, 0.4155, 0.4085,\n",
            "        0.4157, 0.4191, 0.4197, 0.4229, 0.4161, 0.4164, 0.4191, 0.4108, 0.4174,\n",
            "        0.4253, 0.4154, 0.4155, 0.4183, 0.4138, 0.4250, 0.4177, 0.4151, 0.4110,\n",
            "        0.4157, 0.4137, 0.4159, 0.4152, 0.4125, 0.4137, 0.4143, 0.4158, 0.4160,\n",
            "        0.4167, 0.4120, 0.4203, 0.4213, 0.4238, 0.4157, 0.4168, 0.4111, 0.4123,\n",
            "        0.4148, 0.4110, 0.4170, 0.4146, 0.4217, 0.4123, 0.4147, 0.4177, 0.4188,\n",
            "        0.4154, 0.4163, 0.4210, 0.4145, 0.4142, 0.4165, 0.4211, 0.4141, 0.4148,\n",
            "        0.4170, 0.4183, 0.4094, 0.4227, 0.4090, 0.4150, 0.4249, 0.4243, 0.4109,\n",
            "        0.4169, 0.4132, 0.4187, 0.4195, 0.4211, 0.4182, 0.4165, 0.4150, 0.4149,\n",
            "        0.4232, 0.4189, 0.4170, 0.4192, 0.4169, 0.4116, 0.4230, 0.4172, 0.4198,\n",
            "        0.4114, 0.4129, 0.4109, 0.4169, 0.4165, 0.4191, 0.4155, 0.4114, 0.4173,\n",
            "        0.4120, 0.4151, 0.4183, 0.4131, 0.4124, 0.4202, 0.4260, 0.4182, 0.4123,\n",
            "        0.4160, 0.4180, 0.4108, 0.4231, 0.4169, 0.4162, 0.4144, 0.4129, 0.4111,\n",
            "        0.4177, 0.4181, 0.4265, 0.4242, 0.4142, 0.4093, 0.4168, 0.4077, 0.4106,\n",
            "        0.4181, 0.4090, 0.4142, 0.4245, 0.4153, 0.4168, 0.4088, 0.4209, 0.4137,\n",
            "        0.4181, 0.4182, 0.4170, 0.4166, 0.4232, 0.4201, 0.4207, 0.4210, 0.4189,\n",
            "        0.4164, 0.4167, 0.4228, 0.4201, 0.4188, 0.4124, 0.4163, 0.4151, 0.4061,\n",
            "        0.4102, 0.4096, 0.4169, 0.4117, 0.4169, 0.4113, 0.4151, 0.4228, 0.4144,\n",
            "        0.4122, 0.4137, 0.4225, 0.4155, 0.4126, 0.4112, 0.4112, 0.4127, 0.4150,\n",
            "        0.4150, 0.4169, 0.4135, 0.4201, 0.4259, 0.4095, 0.4218, 0.4181, 0.4158,\n",
            "        0.4081, 0.4123, 0.4196, 0.4153, 0.4153], grad_fn=<MaxBackward0>),\n",
            "indices=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
            "torch.Size([13, 20])\n",
            "Parameter containing:\n",
            "tensor([ 1.3508e-01,  1.5439e-01, -1.9350e-01, -6.8777e-02,  1.3787e-01,\n",
            "        -1.8474e-01,  1.2763e-01,  1.8031e-01,  9.5152e-02, -1.2660e-01,\n",
            "         1.4317e-01, -1.4945e-01,  3.4253e-05], requires_grad=True)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# 确定数据\n",
        "torch.random.manual_seed(420)\n",
        "X = torch.rand((500, 20), dtype=torch.float32)\n",
        "y = torch.randint(low=0, high=3, size=(500, 1), dtype=torch.float32)\n",
        "\n",
        "# torch.nn -> nn.Module(层), nn.functional(函数) \n",
        "\n",
        "# 假设我们有500条数据，20个特征，标签分为3类，我们现在要实现一个三层神经网络，这个神经网络的架构如下：\n",
        "# 第一层有13个神经元，第二层有8个神经元，第三层数输出层。其中，第一层激活函数是relu，第二层激活函数是sigmoid\n",
        "class Model(nn.Module):\n",
        "  def __init__(self, in_features=10, out_features=2):\n",
        "\n",
        "    \"\"\"\n",
        "    in_features:输入神经网络的特征数目,输入层上神经元的个数\n",
        "    out_features:神经网络输出的数目，输出层上神经元的个数\n",
        "    \"\"\"\n",
        "    super(Model, self).__init__()\n",
        "    # 隐藏层的第一层\n",
        "    self.linear1 = nn.Linear(in_features, 13, bias=True)\n",
        "    # 隐藏层的第二层\n",
        "    self.linear2 = nn.Linear(13, 8, bias=True)\n",
        "    # 输出层\n",
        "    self.output = nn.Linear(8, out_features, bias=True)\n",
        "\n",
        "  # 神经网络的前向传播\n",
        "  def forward(self, X): \n",
        "    z1 = self.linear1(X)\n",
        "    sigma1 = torch.relu(z1)\n",
        "    z2 = self.linear2(sigma1)\n",
        "    sigma2 = torch.sigmoid(z2)\n",
        "    z3 = self.output(sigma2)\n",
        "    sigma3 = F.softmax(z3, dim=1)\n",
        "    return sigma3\n",
        "\n",
        "input_ = X.shape[1] #特征的数目\n",
        "output_ = len(y.unique()) # 分类的数目\n",
        "\n",
        "# 实例化神经网络类\n",
        "torch.manual_seed(420)\n",
        "net = Model(in_features=input_, out_features=output_)\n",
        "# 向前传播\n",
        "net.forward(X)\n",
        "# 查看输出的标签\n",
        "sigma = net.forward(X)\n",
        "print(sigma.max(axis=1))\n",
        "\n",
        "# 查看每一层上的权重w和截距b\n",
        "print(net.linear1.weight)\n",
        "print(net.linear1.bias)\n"
      ]
    }
  ]
}