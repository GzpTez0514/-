{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch深度学习12-模型训练.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNW6xa3ahA5/VloLPq/FMie",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GzpTez0514/-/blob/main/Pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A012_%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LL3_A-pFzW6",
        "outputId": "d9cfda74-3a31-4d62-f302-b5e295634ee6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.7.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.7.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# 一、设置库，导入环境\n",
        "!pip install torchinfo\n",
        "import os\n",
        "import torch\n",
        "torch.backends.cudnn.benchmark=True # 用于加速GPU计算的代码\n",
        "\n",
        "# 导入pytorch一个完整流程可能所需全部的包\n",
        "import torchvision\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms as T\n",
        "from torchvision import models as m\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 导入作为辅助工具的各类包\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "import datetime\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import gc\n",
        "\n",
        "# 设置全局的随机数种子，这些随机数种子只能提供有限的控制，并不能完全令模型稳定下来\n",
        "torch.manual_seed(1412)\n",
        "random.seed(1412)\n",
        "np.random.seed(1412)\n",
        "\n",
        "# GPU系统会返回true，CPU系统会返回false\n",
        "torch.cuda.is_available()\n",
        "\n",
        "# GPU系统会令device=‘gpu',cpu系统会令device='cpu'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 从本地读取文件\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "    print('User uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))  "
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "lKYDcOZAP-eS",
        "outputId": "534ddf60-568d-4522-88c9-9cb642a17883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d927fdac-a918-4317-b37d-68b59ce486b9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d927fdac-a918-4317-b37d-68b59ce486b9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving test_32x32.mat to test_32x32.mat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-48bf161f-380d-4fa5-b508-2357d0b864c8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-48bf161f-380d-4fa5-b508-2357d0b864c8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving train_32x32.mat to train_32x32.mat\n",
            "User uploaded file \"train_32x32.mat\" with length 182040794 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 二、数据导入、数据探索、数据增强\n",
        "# 通常在第一次导入图像的时候，我们不会使用数据增强的任何手段，而是直接ToTensor()导入进行查看\n",
        "# 导入数据\n",
        "train = torchvision.datasets.SVHN(root=r'C:\\学习资料文件夹\\深兰资料\\数据&论文\\WEEK10-WEEK14-CV数据包\\datasets\\SVHN',\n",
        "                  split='train',\n",
        "                  download=False,\n",
        "                  #transform=T.ToTensor()\n",
        "                  )\n",
        "\n",
        "test = torchvision.datasets.SVHN(root=r'C:\\学习资料文件夹\\深兰资料\\数据&论文\\WEEK10-WEEK14-CV数据包\\datasets\\SVHN',\n",
        "                  split='test',\n",
        "                  download=False,\n",
        "                  transform=T.ToTensor())\n",
        "# 先调一张图像来看看\n",
        "train[0][0]\n",
        "\n",
        "# 检查数据量\n",
        "train\n",
        "test\n",
        "\n",
        "# 查看尺寸等信息\n",
        "for x, y in train:\n",
        "  print(x.shape)\n",
        "  print(y)\n",
        "  break\n",
        "\n",
        "# 标签类别\n",
        "print(np.unique(train.labels))\n",
        "\n",
        "# 让每个数据集随机显示5张图像\n",
        "def plotsample(data):\n",
        "  fig, axs = plt.subplots(1, 5, figsize=(10, 10))\n",
        "  for i in range(5):\n",
        "    num = random.randint(0, len(data)-1) # 选取一个随机数\n",
        "    # 抽取数据中对应的图像对象，make_grid函数可将任意格式的图像通道数升为3，而不改变图像原始的数据\n",
        "    npimg = torchvision.utils.make_grid(data[num][0]).numpy()\n",
        "    nplabel = data[num][1] # 提取标签\n",
        "    # 将图像由(3, weight, height)转化为(weight, height, 3)，并放入imshow函数中读取\n",
        "    axs[i].imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    axs[i].set_title(nplabel) # 给每个子图加上标签\n",
        "    axs[i].axis('off') # 消除每个子图的坐标轴\n",
        "\n",
        "plotsample(train)"
      ],
      "metadata": {
        "id": "I11VuAb_KN0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义用于处理图像的transform\n",
        "# 训练集可能需要数据增强，测试集确不做数据增强，因此要分开定义\n",
        "# 是否需要数据增强呢，一般一开始不会考虑增加数据增强\n",
        "trainT = T.Compose([T.RandomCrop(28), # 随机裁剪\n",
        "           #T.RandomRotation(degrees=[-30, 30]), # 随机旋转\n",
        "           T.ToTensor(),\n",
        "           T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "           # 由于是实拍数据集，使用ImageNet的均值和标准差进行归一化\n",
        "testT = T.Compose([T.CenterCrop(28), \n",
        "          T.ToTensor(),\n",
        "          T.Normalize(mean=[0.458, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "# 正式导入数据\n",
        "train = torchvision.datasets.SVHN(root=r'C:\\学习资料文件夹\\深兰资料\\数据&论文\\WEEK10-WEEK14-CV数据包\\datasets\\SVHN',\n",
        "                  split='train',\n",
        "                  download=False,\n",
        "                  transform=trainT)\n",
        "\n",
        "test = torchvision.datasets.SVHN(root=r'C:\\学习资料文件夹\\深兰资料\\数据&论文\\WEEK10-WEEK14-CV数据包\\datasets\\SVHN',\n",
        "                  split='test',\n",
        "                  download=False,\n",
        "                  transform=testT)\n",
        "\n",
        "plotsample(train) # 查看增强后的数据"
      ],
      "metadata": {
        "id": "alkCeLOHZyRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 三、基于经典架构构筑自己的网络\n",
        "# 基于小型数据集，首先考虑使用各个经典架构中比较浅显、但学习能力又比较强的架构\n",
        "# 比如ResNet18, VGG16, Inception也可以考虑\n",
        "torch.manual_seed(1412)\n",
        "resnet18_ = m.resnet18()\n",
        "vgg16_ = m.vgg16() # VGG本来参数量就很大，因此我个人较少用vgg16_bn\n",
        "print(resnet18_)\n",
        "print(vgg16_)\n",
        "\n",
        "\n",
        "# 小图像尺寸意味着池化层/步长为2的卷积层出现的次数有限，惯例来说只能出现2次，最终的特征图尺寸是7x7\n",
        "class MyResNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.block1 = nn.Sequential(nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "                   resnet18_.bn1,\n",
        "                   resnet18_.relu) # 删除池化层\n",
        "\n",
        "    self.block2 = resnet18_.layer2\n",
        "    self.block3 = resnet18_.layer3\n",
        "\n",
        "    # 自适应平均池化+线性层，此处都与残差网络一致\n",
        "    self.avgpool = resnet18_.avgpool\n",
        "    self.fc = nn.Linear(in_features=256, out_features=10, bias=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.block1(x)\n",
        "    x = self.block2(x)\n",
        "    x = self.block3(x)\n",
        "    x = self.avgpool(x)\n",
        "    x = x.view(-1, 256)\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        "\n",
        "class MyVGG(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    \n",
        "    # 在9层之后增加一个单独的卷积层，再加入池化层，构成(卷积x2 + 池化) + (卷积x3 + 池化)\n",
        "    self.features = nn.Sequential(*vgg16_.features[0:9],# 星号用于解码\n",
        "                    nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.MaxPool2d(2, 2, padding=0, dilation=1, ceil_mode=False)\n",
        "                    )\n",
        "    \n",
        "    # 进入线性层时输入通道发生变化，因此线性层需要重写\n",
        "    # 输出层也需要重写\n",
        "    self.avgpool = vgg16_.avgpool\n",
        "    self.fc = nn.Sequential(nn.Linear(in_features=7*7*128, out_features=4096, bias=True),\n",
        "                 *vgg16_.classifier[1:6],\n",
        "                 nn.Linear(in_features=4096, out_features=10, bias=True))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    x = self.avgpool(x)\n",
        "    x = x.view(-1, 7*7*128)\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        "  \n",
        "from torchinfo import summary\n",
        "print(summary(MyResNet(), (10, 3, 28, 28), depth=1, device='cpu'))\n",
        "print(summary(MyVGG(), (10, 3, 28, 28), depth=1, device='cpu'))\n",
        "\n",
        "# 在这个过程中，我们是从已经实例化的类中直接复制层来使用\n",
        "# 因此我们复用经典架构的部分，参数已经被实例化好了\n",
        "# 因此实例化具体的MyResNet()时没有参数生成\n",
        "print([*MyResNet().block2[0].parameters()][0][0][0])\n",
        "print([*resnet18_.layer2[0].conv1.parameters()][0][0][0])\n",
        "\n",
        "# 没有复用经典架构的部分，则在我们实例化网络的时候才有参数\n",
        "print([*resnet18_.fc.parameters()][0][0])\n",
        "print([*MyResNet().fc.parameters()][0][0])"
      ],
      "metadata": {
        "id": "c_NCKb07iuMi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1040674-ebd4-4d59-c448-33f26c3972b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
            ")\n",
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n",
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "MyResNet                                 [10, 10]                  --\n",
            "├─Sequential: 1-1                        [10, 64, 28, 28]          1,856\n",
            "├─Sequential: 1-2                        [10, 128, 14, 14]         525,568\n",
            "├─Sequential: 1-3                        [10, 256, 7, 7]           2,099,712\n",
            "├─AdaptiveAvgPool2d: 1-4                 [10, 256, 1, 1]           --\n",
            "├─Linear: 1-5                            [10, 10]                  2,570\n",
            "==========================================================================================\n",
            "Total params: 2,629,706\n",
            "Trainable params: 2,629,706\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 2.07\n",
            "==========================================================================================\n",
            "Input size (MB): 0.09\n",
            "Forward/backward pass size (MB): 38.13\n",
            "Params size (MB): 10.52\n",
            "Estimated Total Size (MB): 48.75\n",
            "==========================================================================================\n",
            "==========================================================================================\n",
            "Layer (type:depth-idx)                   Output Shape              Param #\n",
            "==========================================================================================\n",
            "MyVGG                                    [10, 10]                  --\n",
            "├─Sequential: 1-1                        [10, 128, 7, 7]           407,744\n",
            "├─AdaptiveAvgPool2d: 1-2                 [10, 128, 7, 7]           --\n",
            "├─Sequential: 1-3                        [10, 10]                  42,516,490\n",
            "==========================================================================================\n",
            "Total params: 42,924,234\n",
            "Trainable params: 42,924,234\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 1.45\n",
            "==========================================================================================\n",
            "Input size (MB): 0.09\n",
            "Forward/backward pass size (MB): 14.71\n",
            "Params size (MB): 171.70\n",
            "Estimated Total Size (MB): 186.50\n",
            "==========================================================================================\n",
            "tensor([[ 0.0324, -0.0175,  0.0459],\n",
            "        [ 0.0728, -0.0392,  0.0903],\n",
            "        [-0.0266, -0.0177, -0.0227]], grad_fn=<SelectBackward0>)\n",
            "tensor([[ 0.0324, -0.0175,  0.0459],\n",
            "        [ 0.0728, -0.0392,  0.0903],\n",
            "        [-0.0266, -0.0177, -0.0227]], grad_fn=<SelectBackward0>)\n",
            "tensor([-0.0162, -0.0010, -0.0211,  0.0343, -0.0119, -0.0185,  0.0126,  0.0113,\n",
            "         0.0280, -0.0258, -0.0210, -0.0206, -0.0371, -0.0026,  0.0174,  0.0050,\n",
            "        -0.0121,  0.0221, -0.0141,  0.0128, -0.0248, -0.0335, -0.0229,  0.0295,\n",
            "        -0.0341, -0.0223, -0.0072, -0.0324, -0.0038,  0.0150, -0.0193,  0.0186,\n",
            "        -0.0190, -0.0410,  0.0386, -0.0022,  0.0042,  0.0228,  0.0215,  0.0139,\n",
            "         0.0049, -0.0176, -0.0277,  0.0248,  0.0410, -0.0212, -0.0179,  0.0312,\n",
            "         0.0420,  0.0304, -0.0156, -0.0096,  0.0302, -0.0142,  0.0191,  0.0258,\n",
            "         0.0436, -0.0028, -0.0275,  0.0164,  0.0103, -0.0341, -0.0365,  0.0279,\n",
            "        -0.0133, -0.0125, -0.0059, -0.0103, -0.0147, -0.0029,  0.0347, -0.0126,\n",
            "        -0.0303,  0.0190, -0.0019,  0.0093, -0.0252, -0.0141, -0.0147,  0.0247,\n",
            "        -0.0380, -0.0305, -0.0035,  0.0172,  0.0438, -0.0187, -0.0333,  0.0199,\n",
            "        -0.0012,  0.0318,  0.0154,  0.0340,  0.0341, -0.0235, -0.0194, -0.0093,\n",
            "        -0.0096, -0.0215, -0.0216,  0.0244, -0.0183,  0.0257,  0.0034, -0.0424,\n",
            "         0.0010,  0.0401, -0.0320, -0.0388, -0.0201,  0.0361,  0.0011,  0.0029,\n",
            "         0.0285, -0.0391, -0.0212, -0.0347,  0.0040, -0.0138,  0.0104, -0.0054,\n",
            "        -0.0164,  0.0066,  0.0024,  0.0347, -0.0045, -0.0356,  0.0096,  0.0104,\n",
            "         0.0323, -0.0398,  0.0101,  0.0136,  0.0200,  0.0074, -0.0211, -0.0207,\n",
            "         0.0283, -0.0007, -0.0136,  0.0175, -0.0031, -0.0067, -0.0123,  0.0177,\n",
            "        -0.0266,  0.0234, -0.0234,  0.0133, -0.0339, -0.0167, -0.0146, -0.0081,\n",
            "        -0.0242,  0.0067,  0.0125,  0.0078, -0.0226, -0.0063, -0.0417, -0.0411,\n",
            "         0.0094,  0.0342, -0.0081, -0.0166,  0.0337,  0.0275, -0.0230, -0.0144,\n",
            "         0.0215, -0.0035, -0.0228,  0.0170,  0.0152,  0.0264, -0.0353,  0.0318,\n",
            "        -0.0229,  0.0060, -0.0110, -0.0167,  0.0259,  0.0198, -0.0390, -0.0226,\n",
            "        -0.0331, -0.0093,  0.0179, -0.0382,  0.0013,  0.0294,  0.0214, -0.0021,\n",
            "        -0.0437, -0.0428,  0.0356,  0.0001, -0.0200,  0.0423, -0.0236, -0.0145,\n",
            "         0.0433, -0.0038, -0.0341,  0.0116,  0.0200, -0.0326, -0.0213, -0.0015,\n",
            "        -0.0222,  0.0137, -0.0199,  0.0140, -0.0327, -0.0229, -0.0079,  0.0087,\n",
            "         0.0371, -0.0163,  0.0082,  0.0376, -0.0013, -0.0239,  0.0311, -0.0130,\n",
            "         0.0061,  0.0324, -0.0049, -0.0225,  0.0318, -0.0301,  0.0265,  0.0214,\n",
            "        -0.0198,  0.0084,  0.0041,  0.0263, -0.0203,  0.0114,  0.0194, -0.0061,\n",
            "         0.0423,  0.0155,  0.0263, -0.0243,  0.0199,  0.0209, -0.0232, -0.0311,\n",
            "         0.0074,  0.0398,  0.0308,  0.0213,  0.0331, -0.0205, -0.0407, -0.0239,\n",
            "        -0.0079,  0.0079, -0.0266, -0.0060,  0.0286, -0.0303,  0.0080, -0.0385,\n",
            "        -0.0125, -0.0302, -0.0011, -0.0124, -0.0057,  0.0351, -0.0089, -0.0412,\n",
            "         0.0419, -0.0396,  0.0050,  0.0152,  0.0320,  0.0411, -0.0173, -0.0005,\n",
            "        -0.0288, -0.0239, -0.0151, -0.0050,  0.0228,  0.0301,  0.0295, -0.0331,\n",
            "        -0.0023,  0.0431,  0.0177, -0.0224,  0.0133, -0.0318,  0.0341, -0.0310,\n",
            "        -0.0091,  0.0022, -0.0387,  0.0369, -0.0086, -0.0397,  0.0043, -0.0342,\n",
            "         0.0130, -0.0298,  0.0317,  0.0439,  0.0134,  0.0169, -0.0202,  0.0034,\n",
            "        -0.0307,  0.0132,  0.0044,  0.0389, -0.0314,  0.0223,  0.0330, -0.0207,\n",
            "        -0.0065, -0.0260,  0.0266, -0.0105, -0.0428,  0.0157,  0.0347, -0.0412,\n",
            "        -0.0126,  0.0332,  0.0212,  0.0420, -0.0286, -0.0319,  0.0010,  0.0096,\n",
            "         0.0008, -0.0124, -0.0048, -0.0144, -0.0184, -0.0369,  0.0343,  0.0027,\n",
            "        -0.0217, -0.0204, -0.0133, -0.0418,  0.0293, -0.0100,  0.0198,  0.0217,\n",
            "         0.0385,  0.0180, -0.0129,  0.0276,  0.0311, -0.0352, -0.0278, -0.0075,\n",
            "         0.0366,  0.0375, -0.0143, -0.0383, -0.0242, -0.0419, -0.0228, -0.0325,\n",
            "        -0.0086, -0.0282,  0.0016,  0.0013,  0.0364,  0.0049, -0.0052, -0.0401,\n",
            "         0.0341,  0.0278,  0.0373,  0.0095, -0.0140,  0.0035, -0.0140,  0.0295,\n",
            "        -0.0107, -0.0264, -0.0200,  0.0045,  0.0420,  0.0108,  0.0137,  0.0009,\n",
            "         0.0284, -0.0166,  0.0285,  0.0222,  0.0299, -0.0283,  0.0169,  0.0411,\n",
            "         0.0356,  0.0101, -0.0175, -0.0075, -0.0075,  0.0216, -0.0427, -0.0385,\n",
            "         0.0299, -0.0398,  0.0160, -0.0355, -0.0286, -0.0175,  0.0040,  0.0179,\n",
            "        -0.0198, -0.0228, -0.0424,  0.0217, -0.0214,  0.0058, -0.0118, -0.0201,\n",
            "        -0.0286, -0.0269,  0.0161, -0.0186, -0.0311, -0.0123, -0.0023, -0.0064,\n",
            "        -0.0213, -0.0194,  0.0360, -0.0196,  0.0348, -0.0332, -0.0269, -0.0428,\n",
            "         0.0107,  0.0118, -0.0221, -0.0407, -0.0066, -0.0189,  0.0026,  0.0338,\n",
            "         0.0021,  0.0085,  0.0439,  0.0396, -0.0432,  0.0319, -0.0345,  0.0020,\n",
            "        -0.0367,  0.0284, -0.0056,  0.0246,  0.0179,  0.0262, -0.0428,  0.0140,\n",
            "         0.0166,  0.0367, -0.0399, -0.0236, -0.0300,  0.0017, -0.0103, -0.0038,\n",
            "        -0.0171,  0.0361, -0.0397, -0.0009, -0.0404,  0.0161, -0.0248, -0.0419,\n",
            "         0.0308,  0.0010,  0.0298, -0.0434,  0.0314, -0.0296,  0.0255, -0.0025,\n",
            "         0.0326, -0.0174,  0.0025, -0.0379,  0.0312, -0.0095, -0.0314,  0.0036,\n",
            "         0.0434, -0.0278,  0.0293, -0.0332,  0.0421,  0.0119, -0.0276, -0.0092,\n",
            "         0.0072,  0.0065,  0.0300,  0.0224, -0.0341,  0.0348,  0.0244,  0.0409],\n",
            "       grad_fn=<SelectBackward0>)\n",
            "tensor([ 2.1825e-02, -3.8537e-02, -5.4820e-02, -5.1262e-02, -2.1468e-02,\n",
            "        -1.6897e-02,  1.6042e-04, -8.0866e-03, -1.2811e-02, -3.1890e-02,\n",
            "         5.7537e-02,  5.2508e-02, -4.4454e-03,  3.8484e-02,  1.9710e-02,\n",
            "        -2.9129e-02, -4.5182e-02, -1.8556e-02,  6.0332e-02, -3.6209e-02,\n",
            "        -4.5339e-02, -2.8569e-02,  3.5853e-02,  5.9507e-02, -5.7613e-02,\n",
            "        -5.6614e-02,  3.9794e-02,  1.3848e-02, -1.7843e-02, -1.9115e-02,\n",
            "         4.6396e-02,  5.1171e-02, -4.1696e-02,  5.5156e-02,  2.5523e-02,\n",
            "         2.5154e-02, -7.3692e-03, -6.0921e-02,  1.7502e-02, -6.1320e-02,\n",
            "        -2.9313e-02, -2.2339e-02, -6.2270e-03,  4.5804e-02,  1.6640e-02,\n",
            "         6.2471e-02,  5.5956e-02, -3.5882e-02, -4.8028e-03, -4.6679e-02,\n",
            "        -4.2411e-02,  2.9717e-02, -4.1117e-02, -5.6844e-02,  4.9904e-02,\n",
            "         9.0661e-03, -9.4536e-03, -3.9394e-02,  2.3095e-02, -5.6006e-02,\n",
            "         3.7622e-02,  5.5884e-02,  1.1784e-02,  3.1466e-02,  5.9062e-03,\n",
            "        -4.3177e-02, -3.9851e-02,  2.7044e-02,  3.3938e-02,  1.2474e-02,\n",
            "        -7.6819e-03, -5.5991e-02,  3.7088e-02, -3.9392e-02,  2.8552e-02,\n",
            "         4.7551e-02,  4.6015e-03, -4.9212e-02,  7.5724e-03,  4.5744e-02,\n",
            "         3.2816e-02,  2.3931e-02,  5.3327e-02,  6.0014e-02, -1.0553e-02,\n",
            "        -5.0394e-02, -3.1725e-02, -3.4178e-02, -1.5607e-02,  3.7256e-02,\n",
            "         1.4084e-02, -1.7980e-02, -3.1146e-02,  3.4697e-03,  5.9565e-02,\n",
            "         1.0963e-02, -4.4244e-02,  3.9113e-02,  1.0292e-02,  3.5223e-02,\n",
            "         2.2323e-03, -4.3978e-02,  1.0017e-02,  5.9620e-02, -2.5324e-02,\n",
            "         1.7120e-02,  2.5068e-02,  1.1657e-02, -2.4772e-02, -3.2528e-02,\n",
            "         4.8647e-02, -4.3860e-02, -4.5170e-02, -1.7858e-02,  5.3144e-02,\n",
            "         3.0259e-03,  3.1003e-02,  4.7919e-02,  6.2300e-02,  1.5240e-02,\n",
            "        -6.2456e-02,  3.3582e-02,  2.5843e-02, -1.1086e-02,  4.3097e-02,\n",
            "         7.9155e-05, -9.2856e-03, -2.0250e-02,  9.0998e-03,  6.2066e-02,\n",
            "        -2.2076e-02,  2.7759e-04,  2.2524e-02, -1.7153e-02,  4.1813e-02,\n",
            "         1.6157e-02,  2.7219e-02, -4.1237e-03, -2.5959e-02, -6.6061e-03,\n",
            "        -3.9724e-02, -5.2029e-03,  9.9414e-03, -1.8868e-03,  2.5581e-02,\n",
            "        -2.2059e-02,  3.3200e-02, -4.1328e-02,  1.1521e-02,  3.9876e-02,\n",
            "        -4.4021e-02, -2.9897e-02,  5.3564e-02, -5.2795e-03, -3.5780e-02,\n",
            "        -7.8762e-03, -2.1022e-02, -2.4164e-02,  3.2499e-02, -3.2716e-02,\n",
            "         1.0852e-02, -8.8594e-03,  1.9272e-02,  5.1818e-02, -5.7085e-02,\n",
            "         3.8703e-02,  4.5509e-02, -1.5467e-02,  4.4188e-02, -1.3081e-03,\n",
            "         3.0523e-02, -1.1433e-02, -3.8117e-02,  3.2303e-02,  3.6177e-02,\n",
            "        -2.3782e-02, -1.7807e-02,  3.9310e-02,  3.7868e-02, -5.5702e-02,\n",
            "        -1.9725e-02, -4.5219e-02,  3.0526e-02,  3.7316e-02,  4.4075e-02,\n",
            "         4.5734e-03,  2.7643e-02, -2.1438e-02, -3.3334e-02,  5.2204e-02,\n",
            "         3.7417e-02, -7.6540e-03, -1.6735e-02,  4.1732e-02,  1.3276e-02,\n",
            "         5.3102e-02, -4.0030e-02,  2.0512e-02, -6.1941e-02,  2.4735e-02,\n",
            "         5.2217e-02, -1.0809e-02, -2.8390e-02, -4.5983e-02, -2.2926e-02,\n",
            "        -1.7591e-02,  4.0371e-02,  2.0504e-02, -3.8558e-02,  1.3506e-02,\n",
            "        -4.6289e-02,  2.5814e-02, -4.7285e-02,  2.6299e-02, -2.8104e-02,\n",
            "        -2.2899e-02,  5.6969e-02, -5.3134e-02, -1.3005e-02, -5.1277e-02,\n",
            "         2.2411e-02, -2.9611e-02,  2.2178e-02, -3.5760e-02, -4.8113e-02,\n",
            "         7.4274e-03, -3.2695e-02,  1.1255e-02, -4.6921e-02, -4.6146e-02,\n",
            "         1.9876e-02,  4.5499e-02, -4.1659e-02,  1.3786e-02,  3.9343e-02,\n",
            "        -2.4914e-03, -1.1119e-02,  6.1101e-02,  3.8643e-02,  3.5121e-02,\n",
            "        -2.0962e-02,  6.0977e-02, -3.7143e-02,  2.7691e-02, -3.5616e-02,\n",
            "         1.3352e-02, -5.2426e-02,  3.8077e-02, -6.0824e-02, -1.8223e-02,\n",
            "        -3.9008e-02,  3.6912e-02,  1.7625e-02,  4.5152e-03, -6.1166e-02,\n",
            "         4.0589e-02], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 提前停止\n",
        "class EarlyStopping():\n",
        "  '''\n",
        "  在测试集上的损失连续几个epochs不再降低的时候，提前停止\n",
        "  val_loss: 测试集/验证集上这个epoch的损失\n",
        "\n",
        "  '''\n",
        "  def __init__(self, patience=5, tol=0.0005):\n",
        "    '''\n",
        "    patience：连续patience个epochs上损失不再降低的时候，停止迭代\n",
        "    tol：阈值，当新损失与旧损失之前的差异小于tol时，认为模型不再提升\n",
        "\n",
        "    '''\n",
        "    self.patience = patience\n",
        "    self.tol = tol\n",
        "    self.counter = 0 # 连续x次低于tol值\n",
        "    self.lowest_loss = None # 用于记录历史最低损失，在没有最低损失之前为None\n",
        "    self.early_stop = False # 是否触发提前停止\n",
        "\n",
        "  def __call__(self, val_loss):\n",
        "    '''\n",
        "    val_loss：外部输入的实际损失\n",
        "    '''\n",
        "    if self.lowest_loss == None:\n",
        "      self.lowest_loss = val_loss\n",
        "    elif self.lowest_loss - val_loss > self.tol:\n",
        "      self.lowest_loss = val_loss\n",
        "      self.counter = 0\n",
        "    elif self.lowest_loss - val_loss < self.tol:\n",
        "      self.counter += 1\n",
        "      print(f'\\t NOTICE: Early Stopping counter {self.counter} of {self.patience}')\n",
        "    if self.counter >= self.patience:\n",
        "      print('\\t NOTICE: Early Stopping Actived')\n",
        "      self.early_stop = True\n",
        "    return self.early_stop\n"
      ],
      "metadata": {
        "id": "kPb3JmQ0ng4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 训练、测试、监控、保存权重、绘图\n",
        "# 在这个函数中，我们将整合之前所写的全部内容，并将训练、测试、监控、保存权重等流程全部包含在同一个函数中\n",
        "\n",
        "# 训练函数\n",
        "sigma = torch.ones([3, 3]) + np.random.normal(size=(3, 3))\n",
        "print(sigma)\n",
        "yhat = torch.max(sigma, 1)\n",
        "print(yhat[0])\n",
        "print(yhat[1]) # [1, 1, 0]\n",
        "\n",
        "y = torch.tensor([1, 1, 2])\n",
        "print(yhat[1] == y)\n",
        "print((yhat[1] == y).sum())\n",
        "print(((yhat[1] == y).sum()/3), torch.float32)"
      ],
      "metadata": {
        "id": "aGJEV8wdvv0H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4f29b7e-7984-4b89-f7e8-8f85af4260b0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 2.4554, -0.0274, -0.4746],\n",
            "        [ 0.1598,  1.0367,  0.4442],\n",
            "        [-1.1785, -0.2464,  0.0895]], dtype=torch.float64)\n",
            "tensor([2.4554, 1.0367, 0.0895], dtype=torch.float64)\n",
            "tensor([0, 1, 2])\n",
            "tensor([False,  True,  True])\n",
            "tensor(2)\n",
            "tensor(0.6667) torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def IterOnce(net, criterion, opt, x, y):\n",
        "  '''\n",
        "  对模型进行一次迭代的函数\n",
        "\n",
        "  net：实例化后的网络\n",
        "  criterion：损失函数\n",
        "  opt：优化算法\n",
        "  x：这一个batch中所有的样本\n",
        "  y：这一个batch中所有样本的真实标签\n",
        "  sigma：softmax函数返回的对应类别的值\n",
        "\n",
        "  '''\n",
        "  sigma = net.forward(x)\n",
        "  loss = criterion(sigma, y)\n",
        "  loss.backward()\n",
        "  opt.step()\n",
        "  opt.zero_grad(set_to_none=True) # 比起设置梯度为0，让梯度为None会更节约内存\n",
        "  yhat = torch.max(sigma, 1)[1]\n",
        "  correct = torch.sum(yhat == y)\n",
        "  return correct, loss\n",
        "\n",
        "\n",
        "def TestOnce(net, criterion, x, y):\n",
        "  '''\n",
        "  对一组数据进行测试并输出测试结果的函数\n",
        "\n",
        "  net：经过训练后的架构\n",
        "  criterion：损失函数\n",
        "  x：要测试的数据所在的样本\n",
        "  y：要测试的数据的真实标签\n",
        "  对测试，一定要阻止计算图追踪\n",
        "  这样可以节省很多内存，加速运算\n",
        "\n",
        "  '''\n",
        "  with torch.no_grad():\n",
        "    sigma = net.forward(x)\n",
        "    loss = criterion(sigma, y)\n",
        "    yhat = torch.max(sigma, 1)[1]\n",
        "    correct = torch.sum(yhat == y)\n",
        "  return loss, correct\n",
        "\n",
        "\n",
        "def fit_test(net, batchdata, testdata, criterion, opt, epochs, tol, modelname, PATH):\n",
        "  '''\n",
        "  对模型进行训练，并在每个epoch后输出训练集和测试集上的准确率/损失\n",
        "  以现实对模型的监控\n",
        "  实现模型的保存\n",
        "\n",
        "  net：实例化后的网络\n",
        "  batchdata：使用DataLoader分割后的训练数据：\n",
        "  testdata：使用DataLoader分割后的测试数据\n",
        "  criterion：损失函数\n",
        "  opt：优化函数\n",
        "  epochs：一共使用完整数据集epochs次\n",
        "  tol：提前停止时测试集上loss下降的阈值，连续5次loss下降不超过tol就会触发提前停止\n",
        "  modelname：现在正在运行的模型名称，用于保存权重时作为文件名\n",
        "  PATH：将权重文件保存在path目录下\n",
        "\n",
        "  '''\n",
        "  SamplePerEpoch = batchdata.dataset.__len__() # 整个epoch里有多少个样本\n",
        "  allsamples = SamplePerEpoch * epochs\n",
        "  trainedsamples = 0\n",
        "  trainlosslist = []\n",
        "  testlosslist = []\n",
        "  early_stopping = EarlyStopping(tol=tol)\n",
        "  highest_acc = None\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    net.train()\n",
        "    correct_train = 0\n",
        "    loss_train =0\n",
        "    for batch_idx, (x, y) in enumerate(batchdata):\n",
        "      y = y.view(x.shape[0])\n",
        "      correct, loss = IterOnce(net, criterion, opt, x, y)\n",
        "      trainedsamples += x.shape[0]\n",
        "      loss_train += loss\n",
        "      correct_train += correct\n",
        "      if (batch_idx) % 125 == 0:\n",
        "        # 现在进行到了那个epoch\n",
        "        # 现在训练到了多少个样本\n",
        "        # 总共要训练多少个样本\n",
        "        # 现在训练的样本占总共需要训练样本的百分比\n",
        "        print('Epoch{}:[{}/{}({:.0f}%)]'.format(epoch, \n",
        "                            trainedsamples,\n",
        "                            allsamples,\n",
        "                            100*trainedsamples/allsamples))\n",
        "    \n",
        "    TrainAccThisEpoch = float(correct_train*100)/SamplePerEpoch # 当前epoch训练样本的准确率\n",
        "    TrainLossThisEpoch = float(loss_train*100)/SamplePerEpoch # 当前epoch训练样本的损失\n",
        "    trainlosslist.append(TrainLossThisEpoch)\n",
        "\n",
        "    # 每次训练完一个epoch，就在测试集上验证一下模型现在的效果\n",
        "    net.eval()\n",
        "    correct_test = 0\n",
        "    loss_test = 0\n",
        "    TestSample = testdata.dataset.__len__()\n",
        "\n",
        "    for x,y in testdata:\n",
        "      y = y.view(x.shape[0])\n",
        "      correct, loss = TestOnce(net, criterion, x, y)\n",
        "      loss_test += losstest\n",
        "      correct_test += correct\n",
        "\n",
        "    TestAccThisEpoch = float(correct_test*100)/TestSample\n",
        "    TestLossThisEpoch = float(loss_test*100)/TestSample\n",
        "    testlosslist.append(TestLossThisEpoc)\n",
        "\n",
        "    # 对每一个epoch，打印训练和测试的结果\n",
        "    # 训练集上的损失，测试集上的损失，训练集上的准确率，测试集上的准确率\n",
        "    print('\\t Train Loss:{:.6f}, Test Loss:{:.6f}, TrainAcc:{:.3f}%, TestAcc:{:.3f}%'.format(TrainLossThisEpoch,\n",
        "                                                    TestLossThisEpoch,\n",
        "                                                    TrainAccThisEpoch,\n",
        "                                                    TestAccThisEpoch))\n",
        "    # 如果测试集准确率出现新高/测试集loss出现新低，那我们会保存现在的这一组权重\n",
        "    if highestacc == None: # 首次进行测试\n",
        "      highestacc = TestAccThisEpoch\n",
        "    if highestacc < TestAccThisEpoch:\n",
        "      highestacc = TestAccThisEpoch\n",
        "      torch.save(net.state_dict(), os.path.join(PATH， modelname + '.pt'))\n",
        "      print('\\t Weight Saved')\n",
        "    \n",
        "    # 提前停止\n",
        "    early_stop = early_stopping(TestLossThisEpoch)\n",
        "    if early_stop == 'True':\n",
        "      break\n",
        "    \n",
        "  print('Complete')\n",
        "  return trainlosslist, testlosslist"
      ],
      "metadata": {
        "id": "pGqbcrXVQD4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Xtq5mOib2JfA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}