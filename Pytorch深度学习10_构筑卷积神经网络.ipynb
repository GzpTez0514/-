{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GzpTez0514/-/blob/main/Pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A010_%E6%9E%84%E7%AD%91%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzkK_G7AVgaO",
        "outputId": "2a6d728c-fc2d-4fb0-d50e-9859057a6eb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 6, 26, 26])\n",
            "torch.Size([10, 4, 24, 24])\n"
          ]
        }
      ],
      "source": [
        "# 卷积层\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "data = torch.ones(size=(10, 3, 28, 28)) # 10张尺寸为28*28的，拥有3个通道的图像\n",
        "conv1 = nn.Conv2d(in_channels=3, \n",
        "          out_channels=6, # 全部通道的扫描值被合并，6个卷积核形成6个feature map\n",
        "          kernel_size=3 # 表示3x3的卷积核\n",
        "          )\n",
        "\n",
        "conv2 = nn.Conv2d(in_channels=6, # 对下一层网络来说，输入的是上层生成的6个feature map\n",
        "          out_channels=4, # 全部特征图的扫描值被合并，4个卷积核形成4个新的feature map\n",
        "          kernel_size=3)\n",
        "\n",
        "print(conv1(data).shape)\n",
        "print(conv2(conv1(data)).shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFgCfH5zWkKU"
      },
      "outputs": [],
      "source": [
        "# 特征图的尺寸 （H + 2P -K）/ S + 1\n",
        "data = torch.ones(size=(10, 3, 28, 28))\n",
        "conv1 = nn.Conv2d(3, 6, 3)\n",
        "conv2 = nn.Conv2d(6, 4, 3)\n",
        "conv3 = nn.Conv2d(4, 16, 5, stride=2, padding=1) \n",
        "conv4 = nn.Conv2d(16, 3, 5, stride=3, padding=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VGG16的复现\n",
        "# (卷积x2 + 池化) x2 -> (卷积x3 + 池化) x3 —> FC层x3 每组卷积+池化算一个block\n",
        "!pip install torchinfo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torchinfo import summary\n",
        "\n",
        "class VGG16(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    # block1\n",
        "    self.conv1 = nn.Conv2d(3, 64, 3, padding=1)\n",
        "    self.conv2 = nn.Conv2d(64, 64, 3, padding=1)\n",
        "    self.pool1 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "    # block2\n",
        "    self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "    self.conv4 = nn.Conv2d(128, 128, 3, padding=1)\n",
        "    self.pool2 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "    # block3\n",
        "    self.conv5 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "    self.conv6 = nn.Conv2d(256, 256, 3, padding=1)\n",
        "    self.conv7 = nn.Conv2d(256, 256, 3, padding=1)\n",
        "    self.pool3 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "    # block4\n",
        "    self.conv8 = nn.Conv2d(256, 512, 3, padding=1)\n",
        "    self.conv9 = nn.Conv2d(512, 512, 3, padding=1)\n",
        "    self.conv10 = nn.Conv2d(512, 512, 3, padding=1)\n",
        "    self.pool4 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "    # block5\n",
        "    self.conv11 = nn.Conv2d(512, 512, 3, padding=1)\n",
        "    self.conv12 = nn.Conv2d(512, 512, 3, padding=1)\n",
        "    self.conv13 = nn.Conv2d(512, 512, 3, padding=1)\n",
        "    self.pool5 = nn.MaxPool2d(2,2)\n",
        "\n",
        "    # FC层\n",
        "    self.linear1 = nn.Linear(512*7*7, 4096)\n",
        "    self.linear2 = nn.Linear(4096, 4096)\n",
        "    self.linear3 = nn.Linear(4096, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.pool1(x)\n",
        "\n",
        "    x = F.relu(self.conv3(x))\n",
        "    x = F.relu(self.conv4(x))\n",
        "    x = self.pool2(x)\n",
        "\n",
        "    x = F.relu(self.conv5(x))\n",
        "    x = F.relu(self.conv6(x))\n",
        "    x = F.relu(self.conv7(x))\n",
        "    x = self.pool3(x)\n",
        "\n",
        "    x = F.relu(self.conv8(x))\n",
        "    x = F.relu(self.conv9(x))\n",
        "    x = F.relu(self.conv10(x))\n",
        "    x = self.pool4(x)\n",
        "\n",
        "    x = F.relu(self.conv11(x))\n",
        "    x = F.relu(self.conv12(x))\n",
        "    x = F.relu(self.conv13(x))\n",
        "    x = self.pool5(x)\n",
        "    \n",
        "    x = x.view(-1, 512*7*7)\n",
        "\n",
        "    x = F.relu(self.linear1(F.dropout(x, p=0.5)))\n",
        "    x = F.relu(self.linear2(F.dropout(x, p=0.5)))\n",
        "\n",
        "    output = F.softmax(self.linear3(x), dim=1)\n",
        "    return output\n",
        "\n",
        "vgg = VGG16()\n",
        "summary(vgg, input_size=(10, 3, 224, 224), device='cpu')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyxAwt4x2slx",
        "outputId": "b737cfd7-4422-462b-e490-a945642206e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.7/dist-packages (1.7.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "VGG16                                    [10, 10]                  --\n",
              "├─Conv2d: 1-1                            [10, 64, 224, 224]        1,792\n",
              "├─Conv2d: 1-2                            [10, 64, 224, 224]        36,928\n",
              "├─MaxPool2d: 1-3                         [10, 64, 112, 112]        --\n",
              "├─Conv2d: 1-4                            [10, 128, 112, 112]       73,856\n",
              "├─Conv2d: 1-5                            [10, 128, 112, 112]       147,584\n",
              "├─MaxPool2d: 1-6                         [10, 128, 56, 56]         --\n",
              "├─Conv2d: 1-7                            [10, 256, 56, 56]         295,168\n",
              "├─Conv2d: 1-8                            [10, 256, 56, 56]         590,080\n",
              "├─Conv2d: 1-9                            [10, 256, 56, 56]         590,080\n",
              "├─MaxPool2d: 1-10                        [10, 256, 28, 28]         --\n",
              "├─Conv2d: 1-11                           [10, 512, 28, 28]         1,180,160\n",
              "├─Conv2d: 1-12                           [10, 512, 28, 28]         2,359,808\n",
              "├─Conv2d: 1-13                           [10, 512, 28, 28]         2,359,808\n",
              "├─MaxPool2d: 1-14                        [10, 512, 14, 14]         --\n",
              "├─Conv2d: 1-15                           [10, 512, 14, 14]         2,359,808\n",
              "├─Conv2d: 1-16                           [10, 512, 14, 14]         2,359,808\n",
              "├─Conv2d: 1-17                           [10, 512, 14, 14]         2,359,808\n",
              "├─MaxPool2d: 1-18                        [10, 512, 7, 7]           --\n",
              "├─Linear: 1-19                           [10, 4096]                102,764,544\n",
              "├─Linear: 1-20                           [10, 4096]                16,781,312\n",
              "├─Linear: 1-21                           [10, 10]                  40,970\n",
              "==========================================================================================\n",
              "Total params: 134,301,514\n",
              "Trainable params: 134,301,514\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 154.80\n",
              "==========================================================================================\n",
              "Input size (MB): 6.02\n",
              "Forward/backward pass size (MB): 1084.46\n",
              "Params size (MB): 537.21\n",
              "Estimated Total Size (MB): 1627.68\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算LeNet5以及AlexNet各层的感受野的大小\n",
        "!pip install torchinfo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torchinfo import summary\n",
        "\n",
        "data = torch.ones(size=(10, 1, 32, 32))\n",
        "\n",
        "class LeNet5(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "    self.pool1 = nn.MaxPool2d(2, 2)\n",
        "    self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "    self.pool2 = nn.MaxPool2d(2, 2)\n",
        "    self.fc1 = nn.Linear(16*5*5, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.tanh(self.conv1(x))\n",
        "    x = self.pool1(x)\n",
        "    x = self.tanh(self.conv2(x))\n",
        "    x = self.pool2(x)\n",
        "    x = x.view(-1, 16*5*5)\n",
        "    x = F.tanh(self.fc1(x))\n",
        "    output = F.softmax(self.fc2(x), dim=1)\n",
        "    output = F.softmax(x.view(-1, 16*5*5), dim=1)\n",
        "    \n",
        "net = LeNet5()"
      ],
      "metadata": {
        "id": "tOvnS2bcMiF0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d60b07a5-d19e-48e8-e40d-e791d6dc0e53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.7/dist-packages (1.7.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 计算卷积层中参数的数量\n",
        "conv1 = nn.Conv2d(3, 6, 3)\n",
        "conv2 = nn.Conv2d(6, 4, 3)\n",
        "\n",
        "print(conv1.weight.numel())\n",
        "print(conv1.bias.numel())\n",
        "print(conv2.weight.numel())\n",
        "print(conv2.bias.numel())\n",
        "\n",
        "conv3 = nn.Conv2d(4, 16, 5, stride=2, padding=1) #(5*5*4)*16+16\n",
        "conv4 = nn.Conv2d(16, 3, 5, stride=3, padding=2) #(5*5*16)*3+3\n",
        "print(conv3.weight.numel())\n",
        "print(conv4.weight.numel())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_xaelCxu4sr",
        "outputId": "d3dbf3c3-7382-475d-a103-2e6343246e1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "162\n",
            "6\n",
            "216\n",
            "4\n",
            "1600\n",
            "1200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用nn.Sequential\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "data = torch.ones(size=(10, 3, 229, 229))\n",
        "# 不使用类，直接将需要串联的网络、函数等信息写在一个‘序列'里面\n",
        "net = nn.Sequential(nn.Conv2d(3, 6, 3), nn.ReLU(inplace=True),\n",
        "           nn.Conv2d(6, 4, 3), nn.ReLU(inplace=True),\n",
        "           nn.MaxPool2d(2),\n",
        "           nn.Conv2d(4, 16, 5, stride=2, padding=1), nn.ReLU(inplace=True),\n",
        "           nn.Conv2d(16, 3, 5, stride=3, padding=2), nn.ReLU(inplace=True),\n",
        "           nn.MaxPool2d(2))\n",
        "\n",
        "net(data).shape"
      ],
      "metadata": {
        "id": "4sx5qU2-w9r7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f3a1ba9-cba1-4c26-9588-5c71bc18a54f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 3, 9, 9])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用nn.Sequential复现VGG16\n",
        "!pip install torchinfo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchinfo import summary\n",
        "class VGG16(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.features_ = nn.Sequential(nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(inplace=True),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    \n",
        "                    nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(inplace=True),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    \n",
        "                    nn.Conv2d(128, 256, 3, padding=1), nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(256, 256, 3, padding=1), nn.ReLU(inplace=True),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    \n",
        "                    nn.Conv2d(256, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
        "                    nn.MaxPool2d(2),\n",
        "                    \n",
        "                    nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
        "                    nn.Conv2d(512, 512, 3, padding=1), nn.ReLU(inplace=True),\n",
        "                    nn.MaxPool2d(2))\n",
        "    \n",
        "    self.clf_ = nn.Sequential(nn.Dropout(0.5),\n",
        "                  nn.Linear(512*7*7, 4096), nn.ReLU(inplace=True),\n",
        "                  nn.Dropout(0.5),\n",
        "                  nn.Linear(4096, 4096), nn.ReLU(inplace=True),\n",
        "                  nn.Linear(4096, 1000), nn.Softmax(dim=1))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.features_(x)\n",
        "    x = x.view(-1, 512*7*7)\n",
        "    output = self.clf_(x)\n",
        "    return output\n",
        "\n",
        "vgg = VGG16()\n",
        "summary(vgg, input_size=(10, 3, 224, 224), device='cpu')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYuppuEA7por",
        "outputId": "e9dae6aa-1a1d-4b1f-bdb1-517d33bbe3bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.7/dist-packages (1.7.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "VGG16                                    [10, 1000]                --\n",
              "├─Sequential: 1-1                        [10, 512, 7, 7]           --\n",
              "│    └─Conv2d: 2-1                       [10, 64, 224, 224]        1,792\n",
              "│    └─ReLU: 2-2                         [10, 64, 224, 224]        --\n",
              "│    └─Conv2d: 2-3                       [10, 64, 224, 224]        36,928\n",
              "│    └─ReLU: 2-4                         [10, 64, 224, 224]        --\n",
              "│    └─MaxPool2d: 2-5                    [10, 64, 112, 112]        --\n",
              "│    └─Conv2d: 2-6                       [10, 128, 112, 112]       73,856\n",
              "│    └─ReLU: 2-7                         [10, 128, 112, 112]       --\n",
              "│    └─Conv2d: 2-8                       [10, 128, 112, 112]       147,584\n",
              "│    └─ReLU: 2-9                         [10, 128, 112, 112]       --\n",
              "│    └─MaxPool2d: 2-10                   [10, 128, 56, 56]         --\n",
              "│    └─Conv2d: 2-11                      [10, 256, 56, 56]         295,168\n",
              "│    └─ReLU: 2-12                        [10, 256, 56, 56]         --\n",
              "│    └─Conv2d: 2-13                      [10, 256, 56, 56]         590,080\n",
              "│    └─ReLU: 2-14                        [10, 256, 56, 56]         --\n",
              "│    └─Conv2d: 2-15                      [10, 256, 56, 56]         590,080\n",
              "│    └─ReLU: 2-16                        [10, 256, 56, 56]         --\n",
              "│    └─MaxPool2d: 2-17                   [10, 256, 28, 28]         --\n",
              "│    └─Conv2d: 2-18                      [10, 512, 28, 28]         1,180,160\n",
              "│    └─ReLU: 2-19                        [10, 512, 28, 28]         --\n",
              "│    └─Conv2d: 2-20                      [10, 512, 28, 28]         2,359,808\n",
              "│    └─ReLU: 2-21                        [10, 512, 28, 28]         --\n",
              "│    └─Conv2d: 2-22                      [10, 512, 28, 28]         2,359,808\n",
              "│    └─ReLU: 2-23                        [10, 512, 28, 28]         --\n",
              "│    └─MaxPool2d: 2-24                   [10, 512, 14, 14]         --\n",
              "│    └─Conv2d: 2-25                      [10, 512, 14, 14]         2,359,808\n",
              "│    └─ReLU: 2-26                        [10, 512, 14, 14]         --\n",
              "│    └─Conv2d: 2-27                      [10, 512, 14, 14]         2,359,808\n",
              "│    └─ReLU: 2-28                        [10, 512, 14, 14]         --\n",
              "│    └─Conv2d: 2-29                      [10, 512, 14, 14]         2,359,808\n",
              "│    └─ReLU: 2-30                        [10, 512, 14, 14]         --\n",
              "│    └─MaxPool2d: 2-31                   [10, 512, 7, 7]           --\n",
              "├─Sequential: 1-2                        [10, 1000]                --\n",
              "│    └─Dropout: 2-32                     [10, 25088]               --\n",
              "│    └─Linear: 2-33                      [10, 4096]                102,764,544\n",
              "│    └─ReLU: 2-34                        [10, 4096]                --\n",
              "│    └─Dropout: 2-35                     [10, 4096]                --\n",
              "│    └─Linear: 2-36                      [10, 4096]                16,781,312\n",
              "│    └─ReLU: 2-37                        [10, 4096]                --\n",
              "│    └─Linear: 2-38                      [10, 1000]                4,097,000\n",
              "│    └─Softmax: 2-39                     [10, 1000]                --\n",
              "==========================================================================================\n",
              "Total params: 138,357,544\n",
              "Trainable params: 138,357,544\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 154.84\n",
              "==========================================================================================\n",
              "Input size (MB): 6.02\n",
              "Forward/backward pass size (MB): 1084.54\n",
              "Params size (MB): 553.43\n",
              "Estimated Total Size (MB): 1643.99\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# NiN网络的复现\n",
        "!pip install torchinfo\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "from torchinfo import summary\n",
        "\n",
        "data = torch.ones(size=(10, 3, 32, 32))\n",
        "\n",
        "class NiN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.block1 = nn.Sequential(nn.Conv2d(3, 192, 5, padding=2), nn.ReLU(inplace=True),\n",
        "                   nn.Conv2d(192, 160, 1), nn.ReLU(inplace=True),\n",
        "                   nn.Conv2d(160, 96, 1), nn.ReLU(inplace=True),\n",
        "                   nn.MaxPool2d(3, stride=2),\n",
        "                   nn.Dropout(0.25))\n",
        "    \n",
        "    self.block2 = nn.Sequential(nn.Conv2d(96, 192, 5, padding=2), nn.ReLU(inplace=True),\n",
        "                   nn.Conv2d(192, 192, 1), nn.ReLU(inplace=True),\n",
        "                   nn.Conv2d(192, 192, 1), nn.ReLU(inplace=True),\n",
        "                   nn.MaxPool2d(3, stride=2), \n",
        "                   nn.Dropout(0.25))\n",
        "    \n",
        "    self.block3 = nn.Sequential(nn.Conv2d(192, 192, 3, padding=1), nn.ReLU(inplace=True),\n",
        "                   nn.Conv2d(192, 192, 1), nn.ReLU(inplace=True),\n",
        "                   nn.Conv2d(192, 10, 1), nn.ReLU(inplace=True),\n",
        "                   nn.AvgPool2d(7, stride=1),\n",
        "                   nn.Softmax(dim=1))\n",
        "    \n",
        "  def forward(self, x):\n",
        "    output = self.block3(self.block2(self.block1(x)))\n",
        "    return output\n",
        "\n",
        "net = NiN()\n",
        "net(data).shape\n",
        "summary(net, input_size=(10, 3, 32, 32))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1g1Hua3JNfN5",
        "outputId": "1f6a648d-6710-4e5c-cce5-8ea3e69bc68c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.7/dist-packages (1.7.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "NiN                                      [10, 10, 1, 1]            --\n",
              "├─Sequential: 1-1                        [10, 96, 15, 15]          --\n",
              "│    └─Conv2d: 2-1                       [10, 192, 32, 32]         14,592\n",
              "│    └─ReLU: 2-2                         [10, 192, 32, 32]         --\n",
              "│    └─Conv2d: 2-3                       [10, 160, 32, 32]         30,880\n",
              "│    └─ReLU: 2-4                         [10, 160, 32, 32]         --\n",
              "│    └─Conv2d: 2-5                       [10, 96, 32, 32]          15,456\n",
              "│    └─ReLU: 2-6                         [10, 96, 32, 32]          --\n",
              "│    └─MaxPool2d: 2-7                    [10, 96, 15, 15]          --\n",
              "│    └─Dropout: 2-8                      [10, 96, 15, 15]          --\n",
              "├─Sequential: 1-2                        [10, 192, 7, 7]           --\n",
              "│    └─Conv2d: 2-9                       [10, 192, 15, 15]         460,992\n",
              "│    └─ReLU: 2-10                        [10, 192, 15, 15]         --\n",
              "│    └─Conv2d: 2-11                      [10, 192, 15, 15]         37,056\n",
              "│    └─ReLU: 2-12                        [10, 192, 15, 15]         --\n",
              "│    └─Conv2d: 2-13                      [10, 192, 15, 15]         37,056\n",
              "│    └─ReLU: 2-14                        [10, 192, 15, 15]         --\n",
              "│    └─MaxPool2d: 2-15                   [10, 192, 7, 7]           --\n",
              "│    └─Dropout: 2-16                     [10, 192, 7, 7]           --\n",
              "├─Sequential: 1-3                        [10, 10, 1, 1]            --\n",
              "│    └─Conv2d: 2-17                      [10, 192, 7, 7]           331,968\n",
              "│    └─ReLU: 2-18                        [10, 192, 7, 7]           --\n",
              "│    └─Conv2d: 2-19                      [10, 192, 7, 7]           37,056\n",
              "│    └─ReLU: 2-20                        [10, 192, 7, 7]           --\n",
              "│    └─Conv2d: 2-21                      [10, 10, 7, 7]            1,930\n",
              "│    └─ReLU: 2-22                        [10, 10, 7, 7]            --\n",
              "│    └─AvgPool2d: 2-23                   [10, 10, 1, 1]            --\n",
              "│    └─Softmax: 2-24                     [10, 10, 1, 1]            --\n",
              "==========================================================================================\n",
              "Total params: 966,986\n",
              "Trainable params: 966,986\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 2.01\n",
              "==========================================================================================\n",
              "Input size (MB): 0.12\n",
              "Forward/backward pass size (MB): 48.61\n",
              "Params size (MB): 3.87\n",
              "Estimated Total Size (MB): 52.60\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GoogleNet的复现\n",
        "!pip install torchinfo\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchinfo import summary\n",
        "\n",
        "class BasicConv2d(nn.Module):\n",
        "  def __init__(self,in_channels, out_channels, **kwargs):\n",
        "    super().__init__()\n",
        "    self.conv = nn.Sequential(nn.Conv2d(in_channels, out_channels, bias=False, **kwargs), \n",
        "                  nn.BatchNorm2d(out_channels), \n",
        "                  nn.ReLU(inplace=True))\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = self.conv(x)\n",
        "    return x\n",
        "\n",
        "# 测试\n",
        "BasicConv2d(2, 10, kernel_size=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AYdvpdjp__ge",
        "outputId": "972fa708-f707-466a-ccdb-3e268adfd506"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.7/dist-packages (1.7.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BasicConv2d(\n",
              "  (conv): Sequential(\n",
              "    (0): Conv2d(2, 10, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "    (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules.container import Sequential\n",
        "# 接下来，我们需要定义Inception块，由于Inception块是并联结构，存在4个branchs，所以不能够使用nn.Sequential进行打包\n",
        "class Inception(nn.Module):\n",
        "  def __init__(self, in_channels: int,\n",
        "             ch1x1: int, \n",
        "             ch3x3red: int,\n",
        "             ch3x3: int,\n",
        "             ch5x5red: int,\n",
        "             ch5x5: int,\n",
        "             pool_proj: int):\n",
        "    super().__init__()\n",
        "    self.branch1 = BasicConv2d(in_channels, ch1x1, kernel_size=1)\n",
        "    self.branch2 = nn.Sequential(BasicConv2d(in_channels, ch3x3red, kernel_size=1), \n",
        "                   BasicConv2d(ch3x3red, ch3x3, kernel_size=3, padding=1))\n",
        "\n",
        "    self.branch3 = nn.Sequential(BasicConv2d(in_channels, ch5x5red, kernel_size=1), \n",
        "                   BasicConv2d(ch5x5red, ch5x5, kernel_size=5, padding=2))\n",
        "\n",
        "    self.branch4 = nn.Sequential(nn.MaxPool2d(kernel_size=3, stride=1), \n",
        "                   BasicConv2d(in_channels, pool_proj, kernel_size=1))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    branch1 = self.branch1(x)\n",
        "    branch2 = self.branch2(x)\n",
        "    branch3 = self.branch3(x)\n",
        "    branch4 = self.branch4(x)\n",
        "    outputs = [branch1, branch2, branch3, branch4]\n",
        "    return torch.cat(outputs, dim=1)\n",
        "\n",
        "# 测试\n",
        "Inception(256, 64, 96, 128, 16, 32, 32) #这是inception3a的参数"
      ],
      "metadata": {
        "id": "5cq41I8oIC-U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e34ae9f0-51c4-44d4-cbed-34ecafee3381"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Inception(\n",
              "  (branch1): BasicConv2d(\n",
              "    (conv): Sequential(\n",
              "      (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (branch2): Sequential(\n",
              "    (0): BasicConv2d(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2d(256, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicConv2d(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (branch3): Sequential(\n",
              "    (0): BasicConv2d(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicConv2d(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (branch4): Sequential(\n",
              "    (0): MaxPool2d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "    (1): BasicConv2d(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 辅助分类器\n",
        "class AuxClf(nn.Module):\n",
        "  def __init__(self,in_channels, num_classes, **kwargs):\n",
        "    super().__init__()\n",
        "    self.features_ = nn.Sequential(nn.AvgPool2d(kernel_size=5, stride=3), \n",
        "                    BasicConv2d(in_channels, 128, kernel_size=1))\n",
        "    \n",
        "    self.clf_ = nn.Sequential(nn.Linear(4*4*128, 1024), \n",
        "                  nn.ReLU(inplace=True),\n",
        "                  nn.Dropout(0.7),\n",
        "                  nn.Linear(1024, num_classes))\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = self.features_(x)\n",
        "    x = x.view(-1, 4*4*128)\n",
        "    x = self.clf_(x)\n",
        "    return x\n",
        "\n",
        "# 测试\n",
        "AuxClf(512, 1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jp6P2CLovHGN",
        "outputId": "ed8d54ec-79a5-4928-c32f-2b9cfa010280"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AuxClf(\n",
              "  (features_): Sequential(\n",
              "    (0): AvgPool2d(kernel_size=5, stride=3, padding=0)\n",
              "    (1): BasicConv2d(\n",
              "      (conv): Sequential(\n",
              "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (2): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (clf_): Sequential(\n",
              "    (0): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.7, inplace=False)\n",
              "    (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义好三个单独的类之后，我们再实现GoogleNet的完整架构\n",
        "class GoogleNet(nn.Module):\n",
        "  def __init__(self, num_classes=1000, blocks=None):\n",
        "    super().__init__()\n",
        "    if blocks is None:\n",
        "      blocks = [BasicConv2d, Inception, AuxClf]\n",
        "    conv_block = blocks[0]\n",
        "    inception_block = blocks[1]\n",
        "    aux_clf_block = blocks[2]\n",
        "\n",
        "    # block1\n",
        "    self.conv1 = conv_block(3, 64, kernel_size=7, stride=2, padding=3)\n",
        "    self.maxpool1 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
        "\n",
        "    # block2\n",
        "    self.conv2 = conv_block(64, 64, kernel_size=1)\n",
        "    self.conv3 = conv_block(64, 192, kernel_size=3, padding=1)\n",
        "    self.maxpool2 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
        "\n",
        "    # block3\n",
        "    self.inception3a = inception_block(192, 64, 96, 128, 16, 32, 32)\n",
        "    self.inception3b = inception_block(256, 128, 128, 192, 32, 96, 64)\n",
        "    self.maxpool3 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
        "\n",
        "    # block4\n",
        "    self.inception4a = inception_block(480, 192, 96, 208, 16, 48, 64)\n",
        "    self.inception4b = inception_block(512, 160, 112, 224, 24, 64, 64)\n",
        "    self.inception4c = inception_block(512, 128, 128, 256, 24, 64, 64)\n",
        "    self.inception4d = inception_block(512, 112, 144, 288, 32, 64, 64)\n",
        "    self.inception4e = inception_block(528, 256, 160, 320, 32, 128, 128)\n",
        "    self.maxpool4 = nn.MaxPool2d(3, stride=2, ceil_mode=True)\n",
        "\n",
        "    # block5\n",
        "    self.inception5a = inception_block(832, 256, 160, 320, 32, 128, 128)\n",
        "    self.inception5b = inception_block(832, 384, 192, 384, 48, 128, 128)\n",
        "\n",
        "    # auxclf\n",
        "    self.aux1 = aux_clf_block(512, num_classes)\n",
        "    self.aux2 = aux_clf_block(528, num_classes)\n",
        "\n",
        "    # clf\n",
        "    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "    # 自适应平局池化， 可以自动为我们输出(1, 1)尺寸的特征图\n",
        "    # 在这里就相当于全局平均池化了\n",
        "    self.dropout = nn.Dropout(0.4)\n",
        "    self.fc = nn.Linear(1024, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # block1\n",
        "    x = self.conv1(x)\n",
        "    x = self.maxpool1(x)\n",
        "\n",
        "    # block2\n",
        "    x = self.conv2(x)\n",
        "    x = self.conv3(x)\n",
        "    x = self.maxpool2(x)\n",
        "\n",
        "    # block3\n",
        "    x = self.inception3a(x)\n",
        "    x = self.inception3b(x)\n",
        "    x = self.maxpool3(x)\n",
        "\n",
        "    # block4\n",
        "    x = self.inception4a(x)\n",
        "    aux1 = self.aux1(x)\n",
        "\n",
        "    x = self.inception4b(x)\n",
        "    x = self.inception4c(x)\n",
        "    x = self.inception4d(x)\n",
        "    aux2 = self.aux2(x)\n",
        "\n",
        "    x = self.inception4e(x)\n",
        "    x = self.maxpool4(x)\n",
        "\n",
        "    # block5\n",
        "    x = self.inception5a(x)\n",
        "    x = self.inception5b(x)\n",
        "\n",
        "    # clf\n",
        "    x = self.avgpool(x)\n",
        "    x = torch.view(-1, )\n",
        "    x = self.dropout(x)\n",
        "    x = self.fc(x)\n",
        "\n",
        "    return x, aux2, aux1"
      ],
      "metadata": {
        "id": "Yiv6G3aR00em"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 复现残差网络(3x3卷积层和1x1卷积层)，每个卷积层后都要跟上BN层，而BN层上可以完成参数初始化\n",
        "!pip install torchinfo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from typing import Type, Union, List, Optional\n",
        "from torchinfo import summary\n",
        "\n",
        "def conv3x3(in_, out_, stride=1, initialzero=False):\n",
        "  bn = nn.BatchNorm2d(out_)\n",
        "  if initialzero == True:\n",
        "    nn.init.constant_(bn.weight, 0)\n",
        "  return nn.Sequential(nn.Conv2d(in_, out_, kernel_size=3, stride=stride, padding=1, bias=False), bn)\n",
        "  # kernel_size一定是3，搭配的padding一定是1，我们将这些参数写死，确保这些参数无法进行修改\n",
        "\n",
        "def conv1x1(in_, out_, stride=1, initialzero=False):\n",
        "  bn = nn.BatchNorm2d(out_)\n",
        "  if initialzero == False:\n",
        "    nn.init.constant_(bn.weight, 0)\n",
        "  return nn.Sequential(nn.Conv2d(in_, out_, kernel_size=1, stride=stride, padding=0, bias=False), bn)\n",
        "  # kernel_size一定是1，搭配的padding一定是0\n",
        "\n",
        "# 查看函数返回的结果，虽然我们定义的是函数，但实际上最后返回的是一个nn.Sequential类\n",
        "conv1x1(2, 10, True)\n",
        "\n",
        "# 测试，initialzero参数是否有效\n",
        "# 随意设置的参数值\n",
        "conv1x1(2, 10, 1, True)[1].weight\n",
        "conv1x1(2, 10, 1, False)[1].weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo9T5_k0puxa",
        "outputId": "c8b31724-f95b-4bef-dbd0-b9d2b5490064"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.7/dist-packages (1.7.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义基础残差单元类：一个残差单元中只包含两个卷积层和一个加和功能\n",
        "class ResidualUnit_draft(nn.Module):\n",
        "  def __init__(self, in_:int, out_:int): \n",
        "    super().__init__()\n",
        "    \n",
        "    # 拟合部分，输出F(x)\n",
        "    self.fit = nn.Sequential(conv3x3(in_, out_), \n",
        "                 nn.ReLU(inplace=True),\n",
        "                 conv3x3(out_, out_))\n",
        "    \n",
        "    # 最后的H(x)需要使用的ReLU函数\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    fx = self.fit_(x) #拟合结果\n",
        "    x = x # 跳跃连接\n",
        "    hx = self.relu(fx + x)\n",
        "    return hx"
      ],
      "metadata": {
        "id": "h0MfHoU_wmz4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 考虑初始化和步长\n",
        "class ResidualUnit_draft(nn.Module):\n",
        "  def __init__(self, in_:int, out_:int, stride1:int=1):\n",
        "    super().__init__()\n",
        "    self.stride1 = stride1\n",
        "\n",
        "    # 拟合部分，输出F(x)\n",
        "    self.fit_ = nn.Sequential(conv3x3(in_, out_, stride=stride1),\n",
        "                  nn.ReLU(inplace=True),\n",
        "                  conv3x3(in_, out_, initialzero=True))\n",
        "    # 跳跃连接，输出x\n",
        "    self.skipconv = conv1x1(in_, out_, stride=stride1)\n",
        "    # 最后的H(x)需要使用的ReLU函数\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    fx = self.fit_(x) # 拟合结果\n",
        "    if self.stride1 != 1:\n",
        "      x = self.skipconv(x) # 跳跃连接\n",
        "    hx = self.relu(fx + x) \n",
        "    return hx"
      ],
      "metadata": {
        "id": "5Z8xXaly25Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 特征图尺寸的变化，消去参数in_，保留out_，就可以将现在写的残差单元类和之后要写的瓶颈架构类打包在一个类中\n",
        "class ResidualUnit(nn.Module):\n",
        "  # 根据是否能将特征图折半，执行不同流程的残差单元\n",
        "  # 如果将特征特尺寸折半，则将输出特征图尺寸翻倍，并在skipconnection上放置1x1卷积网络\n",
        "  # 反之，则不改变特征图尺寸，也不使用任何1x1卷积网络\n",
        "  def __init__(self, out_, stride1:int=1, in_:Optional[int]=None):\n",
        "    super().__init__()\n",
        "    self.stride1 = stride1 # stride1:第一个卷积层/跳跃层中1x1卷积的步长\n",
        "    if stride1 != 1:\n",
        "      in_ = int(out_ / 2)\n",
        "    else:\n",
        "      in_ = out_\n",
        "    \n",
        "    self.fit_ = nn.Sequential(conv3x3(in_, out_, stride1),\n",
        "                  nn.ReLU(inplace=True),\n",
        "                  conv3x3(out_, out_, initialzero=True))\n",
        "    \n",
        "    self.skipconv = conv1x1(in_, out_, stride1)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    fx = self.fit_(x)\n",
        "    if self.stride1 != 1:\n",
        "      x = self.skipconv(x)\n",
        "    hx = self.relu(fx + x)\n",
        "    return hx\n",
        "\n",
        "# 测试\n",
        "data = torch.ones(size=(10, 64, 56, 56))\n",
        "conv3_x_18_0 = ResidualUnit(128, stride1=2) # 特征图尺寸折半，特征图数量加倍\n",
        "print(conv3_x_18_0(data).shape)\n",
        "\n",
        "conv2_x_18_0 = ResidualUnit(64) # 特征图尺寸不变，特征图数量也不变\n",
        "print(conv2_x_18_0(data).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMz2PwJo-tiH",
        "outputId": "ebcb9397-1ba4-4128-94de-c3464e764bc1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 128, 28, 28])\n",
            "torch.Size([10, 64, 56, 56])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 瓶颈架构的基础架构复现\n",
        "class Bottleneck(nn.Module):\n",
        "  def __init__(self, middle_out:int, stride1:int=1, in_:Optional[int]=None):\n",
        "    '''\n",
        "    in_: 输入瓶颈结构的特征图的数量，仅在conv1之后紧跟的瓶颈结构才进行填写，其他时候不填写\n",
        "    stride1: 第一个卷积层/跳跃连接中1x1卷积层的步长\n",
        "\n",
        "    '''\n",
        "\n",
        "    super().__init__()\n",
        "    self.stride1 = stride1\n",
        "    \n",
        "    # 最终的输出量 = 中间输出量的4倍\n",
        "    out_ = middle_out * 4\n",
        "\n",
        "    if in_ == None:\n",
        "      # 不是conv1后紧跟的第一个瓶颈结构\n",
        "      # 需要缩小特征图：输入量 = 中间量 * 2\n",
        "      # 不需要缩小特征图，输入量 = 中间输出量 * 4\n",
        "      if stride1 != 1:\n",
        "        in_ = middle_out * 2\n",
        "      else:\n",
        "        in_ = middle_out * 4\n",
        "\n",
        "    self.fit_ = nn.Sequential(conv1x1(in_, middle_out, stride1),\n",
        "                  nn.ReLU(inplace=True),\n",
        "                  conv3x3(middle_out, middle_out),\n",
        "                  nn.ReLU(inplace=True),\n",
        "                  conv1x1(middle_out, out_, initialzero=True))\n",
        "                  # 最后一个1x1卷积层的输出一定是输入的4倍\n",
        "   \n",
        "    self.skipconv = conv1x1(in_, out_, stride1)\n",
        "    self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    fx = self.fit_(x)\n",
        "    # 对瓶颈架构而言，输入x和输出F(x)的特征图的数量一定不一致，因此x必须要经过1x1卷积层的池化\n",
        "    x = self.skipconv(x)\n",
        "    hx = self.relu(x + fx)\n",
        "    return hx\n",
        "\n",
        "# 测试\n",
        "data1 = torch.ones(10, 64, 56, 56)\n",
        "# 是conv1后紧跟的第一个瓶颈结构\n",
        "conv2_x_101_0 = Bottleneck(in_=64, middle_out=64)\n",
        "print(conv2_x_101_0(data1).shape)\n",
        "\n",
        "data2 = torch.ones(10, 256, 56, 56)\n",
        "# 不是conv1后紧跟的第一个瓶颈结构，但是需要缩小特征图尺寸\n",
        "conv3_x_101_0 = Bottleneck(middle_out=128, stride1=2)\n",
        "print(conv3_x_101_0(data2).shape) # 输出翻2倍并缩小特征图尺寸至一半\n",
        "\n",
        "data3 = torch.ones(10, 512, 28, 28)\n",
        "# 不是conv1后紧跟的第一个瓶颈结构，也不需要缩小特征图尺寸\n",
        "conv3_x_101_1 = Bottleneck(128)\n",
        "print(conv3_x_101_1(data3).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgrO5vTWIS92",
        "outputId": "7854a653-2fb9-48e4-eb04-09da14b069fc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 256, 56, 56])\n",
            "torch.Size([10, 512, 28, 28])\n",
            "torch.Size([10, 512, 28, 28])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Pytorch深度学习10-构筑卷积神经网络.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMBQgATLELoVuozHykXftL2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}