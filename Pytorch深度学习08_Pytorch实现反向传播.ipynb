{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch深度学习08.Pytorch实现反向传播",
      "provenance": [],
      "authorship_tag": "ABX9TyOyvxNceZmuH+5XxT2y3aQU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GzpTez0514/-/blob/main/Pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A008_Pytorch%E5%AE%9E%E7%8E%B0%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yURSUPbBN3j1",
        "outputId": "92ed9f35-2c8b-4223-9d1c-fdeaeb32557a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor(2.),)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "X = torch.tensor(1., requires_grad=True) # requires_grad 表示允许对X进行梯度计算\n",
        "y = X ** 2\n",
        "\n",
        "grad = torch.autograd.grad(y, X) # 这里返回的是在函数y = X ** 2上，X = 1时的导数值\n",
        "print(grad)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 对于单层神经网络，autograd.grad会非常有效。但深层神经网络就不太适合使用grad函数了\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "torch.manual_seed(420)\n",
        "X = torch.rand((500, 20), dtype=torch.float32) * 100\n",
        "y = torch.randint(low=0, high=3, size=(500, 1), dtype=torch.float32)\n",
        "\n",
        "# 定义神经网络的架构\n",
        "class Model(nn.Module):\n",
        "  def __init__(self, in_features=10, out_features=2):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(in_features, 13, bias=True)\n",
        "    self.linear2 = nn.Linear(13, 8, bias=True)\n",
        "    self.output = nn.Linear(8, out_features, bias=True)\n",
        "    \n",
        "  def forward(self, X):\n",
        "    z1 = self.linear1(X)\n",
        "    sigma1 = torch.relu(z1)\n",
        "    z2 = self.linear2(sigma1)\n",
        "    sigma2 = torch.sigmoid(z2)\n",
        "    z3 = self.output(sigma2)\n",
        "  # sigma3 = F.softmax(z3, dim=1)\n",
        "    return z3\n",
        "\n",
        "input_ = X.shape[1] # 特征的数目\n",
        "output_ = len(y.unique()) # 分类的数目\n",
        "\n",
        "# 实例化神经网络类\n",
        "torch.manual_seed(420)\n",
        "net = Model(in_features=input_, out_features=output_)\n",
        "# 正向传播\n",
        "zhat = net.forward(X)\n",
        "# 定义损失函数\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# 对于打包好的CrossEntropyLoss而言，只需要输入zhat\n",
        "loss = criterion(zhat, y.reshape(500).long())\n",
        "print(loss)\n",
        "print(net.linear1.weight.grad) # 不会返回任何值\n",
        "# 反向传播，backward是任意损失函数类都可以调用的方法，对任意损失函数，backward都会求解其中全部w的梯度\n",
        "loss.backward()\n",
        "print(net.linear1.weight.grad) # 返回响应的梯度\n",
        "\n",
        "# 与可以重复进行的正向传播不同，一次正向传播后，反向传播只能进行一次\n",
        "# 如果希望可以重复进行反向传播，可以在第一次进行反向传播的时候加上参数retain_graph\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMNl-ATpPrjx",
        "outputId": "ef733282-4b21-4eee-c9c4-b5d83e1dc88d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.1057, grad_fn=<NllLossBackward0>)\n",
            "None\n",
            "tensor([[ 3.3727e-04,  8.3354e-05,  4.0867e-04,  4.3058e-05,  1.4551e-04,\n",
            "          6.5092e-05,  3.7088e-04,  2.8794e-04,  1.0495e-04,  4.7446e-05,\n",
            "          8.8153e-05,  1.6899e-04,  1.0251e-04,  3.6197e-04,  1.2129e-04,\n",
            "          7.2405e-05,  1.4479e-04,  4.9114e-06,  1.0770e-04,  9.5156e-05],\n",
            "        [ 8.2042e-03,  2.1974e-02,  2.1073e-02,  1.3896e-02,  2.2161e-02,\n",
            "          1.5936e-02,  1.6537e-02,  2.0259e-02,  1.9655e-02,  1.4728e-02,\n",
            "          1.9212e-02,  2.0086e-02,  1.8295e-02,  8.4132e-03,  1.8036e-02,\n",
            "          1.9979e-02,  2.0966e-02,  2.4730e-02,  9.3876e-03,  1.7475e-02],\n",
            "        [ 9.1603e-03,  2.4275e-02,  2.3446e-02,  2.0096e-02,  2.5360e-02,\n",
            "          1.7406e-02,  3.2555e-02,  2.2461e-02,  3.6793e-03,  2.7445e-02,\n",
            "          2.1181e-02,  2.7724e-02,  1.7115e-02,  1.6943e-02,  1.7249e-02,\n",
            "          3.3173e-02,  1.5115e-02,  3.0874e-02,  1.8391e-02,  2.4201e-02],\n",
            "        [-2.8595e-04,  1.2968e-03,  1.3652e-03, -5.6692e-05, -1.7480e-03,\n",
            "         -2.6459e-03,  3.7307e-04, -2.7976e-03,  1.7848e-03, -9.9289e-04,\n",
            "         -3.3944e-04,  2.2783e-04,  1.2076e-03, -3.2906e-04,  5.4641e-04,\n",
            "         -2.8959e-03, -2.7890e-03, -3.0774e-03, -3.8981e-03, -6.0863e-03],\n",
            "        [ 1.5692e-03,  1.3161e-03, -9.0900e-04, -1.0158e-03, -4.9426e-03,\n",
            "          1.3061e-03, -6.8428e-03, -1.0955e-02,  4.2490e-03, -9.7841e-03,\n",
            "         -3.5231e-03, -6.9716e-03,  1.0742e-02,  2.9732e-03, -2.2283e-03,\n",
            "         -4.3101e-03, -1.2823e-03, -6.0040e-03, -1.5857e-03, -3.2208e-03],\n",
            "        [-2.3456e-03,  3.0270e-04, -9.7571e-04, -1.7812e-03, -1.9554e-03,\n",
            "          4.7728e-04, -7.5906e-04, -9.3554e-04, -8.6828e-04, -4.9501e-04,\n",
            "         -3.2291e-03, -2.6119e-03, -9.4872e-04, -1.3638e-03, -1.7115e-03,\n",
            "         -9.1220e-04, -1.0843e-03, -8.5492e-04,  1.5598e-04, -2.5193e-03],\n",
            "        [-1.8459e-02, -2.0009e-02, -1.3430e-02, -1.5675e-02, -1.3660e-02,\n",
            "         -1.4804e-02, -8.0083e-03, -2.0027e-02, -2.9173e-02,  7.5748e-03,\n",
            "         -1.8572e-02, -5.4350e-03, -3.2866e-02, -9.2504e-03, -1.8047e-02,\n",
            "         -9.1732e-03, -1.6036e-02, -1.4584e-02, -8.9602e-03,  1.7230e-03],\n",
            "        [ 9.5595e-04, -2.0620e-02, -2.6421e-02, -1.9139e-02, -2.5206e-02,\n",
            "         -9.0114e-03, -2.5945e-02, -1.8858e-02, -3.5860e-03, -3.2187e-02,\n",
            "         -1.8400e-02, -1.8522e-02, -1.1765e-02, -1.5216e-02, -8.4282e-03,\n",
            "         -3.0337e-02, -1.1364e-02, -2.9159e-02, -5.1049e-03, -2.6377e-02],\n",
            "        [ 3.7390e-03, -1.3553e-03, -2.2382e-03,  2.2693e-03, -1.4464e-03,\n",
            "         -2.4332e-03, -3.7881e-03,  7.3999e-04,  1.2527e-02, -1.9007e-03,\n",
            "          7.8561e-03, -9.5000e-03,  9.1147e-03, -2.2882e-03,  5.5781e-03,\n",
            "         -5.0263e-03,  4.8214e-03,  2.0531e-03,  4.1112e-04, -2.4565e-03],\n",
            "        [ 2.4338e-03,  2.9046e-03, -1.0326e-02, -4.4877e-03, -8.1686e-05,\n",
            "         -3.0232e-03,  3.2874e-03, -7.5255e-03, -1.0000e-02,  3.9967e-03,\n",
            "          6.3736e-03, -2.3521e-03, -3.5956e-03, -4.8027e-03, -2.7523e-03,\n",
            "          1.6497e-03, -1.3330e-03,  3.1406e-03,  4.9480e-03, -1.5097e-03],\n",
            "        [-7.3584e-04,  9.5657e-04,  1.6132e-03,  2.9669e-03, -1.5606e-03,\n",
            "          2.1566e-03, -5.9970e-04,  1.0974e-03,  2.8536e-03, -1.5761e-03,\n",
            "         -1.1973e-03,  1.4157e-03,  1.0819e-03, -1.6395e-04,  1.4259e-03,\n",
            "         -1.5993e-03,  4.9925e-04,  2.4320e-03,  2.8718e-03,  7.4142e-04],\n",
            "        [-1.3084e-02, -1.9413e-02, -3.0946e-02, -2.5855e-02, -2.5361e-02,\n",
            "         -1.3386e-02, -1.6360e-02, -1.5787e-02, -2.4764e-02, -2.0017e-02,\n",
            "         -1.5216e-02, -1.7701e-02, -1.4559e-02, -2.9177e-02, -1.7573e-02,\n",
            "         -1.8530e-02, -1.0069e-02, -9.5260e-03, -1.8987e-02, -1.7839e-02],\n",
            "        [ 4.8107e-04,  7.2480e-04,  9.8633e-05,  8.6471e-04,  1.9964e-03,\n",
            "          1.6202e-03,  1.1736e-03,  2.0290e-03,  3.3147e-04,  3.0171e-03,\n",
            "          1.0456e-03,  1.1400e-03,  5.0297e-04,  3.3159e-04,  1.9707e-03,\n",
            "          3.4884e-04,  8.4107e-04,  2.6777e-03,  8.6282e-04,  5.8412e-04]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 在Pytorch中实现动量法\n",
        "# 恢复小步长\n",
        "lr = 0.1\n",
        "gamma = 0.9\n",
        "\n",
        "dw = net.linear1.weight.grad\n",
        "w = net.linear1.weight.data\n",
        "print(dw)\n",
        "print(w)\n",
        "# v要能够与dw相减，因此必须保持与w相同的结构\n",
        "v = torch.zeros(dw.shape[0], dw.shape[1])\n",
        "v = gamma * v - lr * dw\n",
        "w = w - v\n",
        "print(w)"
      ],
      "metadata": {
        "id": "XFcJyxWFGl4Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20b7ed27-6a88-4877-a8f6-db2c3fee2d45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 3.3727e-04,  8.3354e-05,  4.0867e-04,  4.3058e-05,  1.4551e-04,\n",
            "          6.5092e-05,  3.7088e-04,  2.8794e-04,  1.0495e-04,  4.7446e-05,\n",
            "          8.8153e-05,  1.6899e-04,  1.0251e-04,  3.6197e-04,  1.2129e-04,\n",
            "          7.2405e-05,  1.4479e-04,  4.9114e-06,  1.0770e-04,  9.5156e-05],\n",
            "        [ 8.2042e-03,  2.1974e-02,  2.1073e-02,  1.3896e-02,  2.2161e-02,\n",
            "          1.5936e-02,  1.6537e-02,  2.0259e-02,  1.9655e-02,  1.4728e-02,\n",
            "          1.9212e-02,  2.0086e-02,  1.8295e-02,  8.4132e-03,  1.8036e-02,\n",
            "          1.9979e-02,  2.0966e-02,  2.4730e-02,  9.3876e-03,  1.7475e-02],\n",
            "        [ 9.1603e-03,  2.4275e-02,  2.3446e-02,  2.0096e-02,  2.5360e-02,\n",
            "          1.7406e-02,  3.2555e-02,  2.2461e-02,  3.6793e-03,  2.7445e-02,\n",
            "          2.1181e-02,  2.7724e-02,  1.7115e-02,  1.6943e-02,  1.7249e-02,\n",
            "          3.3173e-02,  1.5115e-02,  3.0874e-02,  1.8391e-02,  2.4201e-02],\n",
            "        [-2.8595e-04,  1.2968e-03,  1.3652e-03, -5.6692e-05, -1.7480e-03,\n",
            "         -2.6459e-03,  3.7307e-04, -2.7976e-03,  1.7848e-03, -9.9289e-04,\n",
            "         -3.3944e-04,  2.2783e-04,  1.2076e-03, -3.2906e-04,  5.4641e-04,\n",
            "         -2.8959e-03, -2.7890e-03, -3.0774e-03, -3.8981e-03, -6.0863e-03],\n",
            "        [ 1.5692e-03,  1.3161e-03, -9.0900e-04, -1.0158e-03, -4.9426e-03,\n",
            "          1.3061e-03, -6.8428e-03, -1.0955e-02,  4.2490e-03, -9.7841e-03,\n",
            "         -3.5231e-03, -6.9716e-03,  1.0742e-02,  2.9732e-03, -2.2283e-03,\n",
            "         -4.3101e-03, -1.2823e-03, -6.0040e-03, -1.5857e-03, -3.2208e-03],\n",
            "        [-2.3456e-03,  3.0270e-04, -9.7571e-04, -1.7812e-03, -1.9554e-03,\n",
            "          4.7728e-04, -7.5906e-04, -9.3554e-04, -8.6828e-04, -4.9501e-04,\n",
            "         -3.2291e-03, -2.6119e-03, -9.4872e-04, -1.3638e-03, -1.7115e-03,\n",
            "         -9.1220e-04, -1.0843e-03, -8.5492e-04,  1.5598e-04, -2.5193e-03],\n",
            "        [-1.8459e-02, -2.0009e-02, -1.3430e-02, -1.5675e-02, -1.3660e-02,\n",
            "         -1.4804e-02, -8.0083e-03, -2.0027e-02, -2.9173e-02,  7.5748e-03,\n",
            "         -1.8572e-02, -5.4350e-03, -3.2866e-02, -9.2504e-03, -1.8047e-02,\n",
            "         -9.1732e-03, -1.6036e-02, -1.4584e-02, -8.9602e-03,  1.7230e-03],\n",
            "        [ 9.5595e-04, -2.0620e-02, -2.6421e-02, -1.9139e-02, -2.5206e-02,\n",
            "         -9.0114e-03, -2.5945e-02, -1.8858e-02, -3.5860e-03, -3.2187e-02,\n",
            "         -1.8400e-02, -1.8522e-02, -1.1765e-02, -1.5216e-02, -8.4282e-03,\n",
            "         -3.0337e-02, -1.1364e-02, -2.9159e-02, -5.1049e-03, -2.6377e-02],\n",
            "        [ 3.7390e-03, -1.3553e-03, -2.2382e-03,  2.2693e-03, -1.4464e-03,\n",
            "         -2.4332e-03, -3.7881e-03,  7.3999e-04,  1.2527e-02, -1.9007e-03,\n",
            "          7.8561e-03, -9.5000e-03,  9.1147e-03, -2.2882e-03,  5.5781e-03,\n",
            "         -5.0263e-03,  4.8214e-03,  2.0531e-03,  4.1112e-04, -2.4565e-03],\n",
            "        [ 2.4338e-03,  2.9046e-03, -1.0326e-02, -4.4877e-03, -8.1686e-05,\n",
            "         -3.0232e-03,  3.2874e-03, -7.5255e-03, -1.0000e-02,  3.9967e-03,\n",
            "          6.3736e-03, -2.3521e-03, -3.5956e-03, -4.8027e-03, -2.7523e-03,\n",
            "          1.6497e-03, -1.3330e-03,  3.1406e-03,  4.9480e-03, -1.5097e-03],\n",
            "        [-7.3584e-04,  9.5657e-04,  1.6132e-03,  2.9669e-03, -1.5606e-03,\n",
            "          2.1566e-03, -5.9970e-04,  1.0974e-03,  2.8536e-03, -1.5761e-03,\n",
            "         -1.1973e-03,  1.4157e-03,  1.0819e-03, -1.6395e-04,  1.4259e-03,\n",
            "         -1.5993e-03,  4.9925e-04,  2.4320e-03,  2.8718e-03,  7.4142e-04],\n",
            "        [-1.3084e-02, -1.9413e-02, -3.0946e-02, -2.5855e-02, -2.5361e-02,\n",
            "         -1.3386e-02, -1.6360e-02, -1.5787e-02, -2.4764e-02, -2.0017e-02,\n",
            "         -1.5216e-02, -1.7701e-02, -1.4559e-02, -2.9177e-02, -1.7573e-02,\n",
            "         -1.8530e-02, -1.0069e-02, -9.5260e-03, -1.8987e-02, -1.7839e-02],\n",
            "        [ 4.8107e-04,  7.2480e-04,  9.8633e-05,  8.6471e-04,  1.9964e-03,\n",
            "          1.6202e-03,  1.1736e-03,  2.0290e-03,  3.3147e-04,  3.0171e-03,\n",
            "          1.0456e-03,  1.1400e-03,  5.0297e-04,  3.3159e-04,  1.9707e-03,\n",
            "          3.4884e-04,  8.4107e-04,  2.6777e-03,  8.6282e-04,  5.8412e-04]])\n",
            "tensor([[ 1.3656e-01, -1.3459e-01,  2.1281e-01, -1.7763e-01, -6.8218e-02,\n",
            "         -1.5410e-01,  1.7245e-01,  8.3885e-02, -1.1153e-01, -1.7294e-01,\n",
            "         -1.2947e-01, -4.3138e-02, -1.1413e-01,  1.6295e-01, -9.4082e-02,\n",
            "         -1.4629e-01, -6.8982e-02, -2.1836e-01, -1.0859e-01, -1.2199e-01],\n",
            "        [ 4.8127e-02,  1.8186e-01,  2.4149e-02, -1.3032e-01,  9.2056e-02,\n",
            "         -9.5202e-02, -1.0584e-01, -4.2852e-02, -1.1669e-01,  2.4581e-02,\n",
            "          1.8152e-01,  3.0500e-02,  1.3506e-01, -1.9425e-01, -1.7591e-01,\n",
            "         -2.9751e-02,  2.0485e-04,  1.3957e-01, -1.9666e-01,  9.3293e-02],\n",
            "        [-1.9192e-01,  3.6070e-02,  1.4778e-01,  3.0845e-02,  7.1393e-02,\n",
            "          1.4217e-01,  2.2122e-01, -1.4032e-01,  7.3255e-02,  1.8409e-01,\n",
            "          1.2716e-01, -2.0253e-01, -1.5509e-01, -2.1899e-01,  9.8980e-02,\n",
            "          2.2123e-01, -2.1659e-01,  1.7880e-01, -2.0922e-01, -2.7275e-02],\n",
            "        [ 1.8144e-01, -3.5166e-02,  2.4801e-02,  1.6299e-01, -1.8755e-01,\n",
            "          5.6587e-02, -1.0911e-01,  2.0523e-01, -1.9378e-01,  1.6899e-02,\n",
            "          1.3966e-01, -1.3137e-01, -1.3201e-01,  7.6554e-02, -1.7558e-01,\n",
            "          1.3096e-01,  2.7182e-02, -2.2010e-01,  7.6883e-02, -1.8731e-01],\n",
            "        [ 2.7419e-02,  1.3699e-01, -3.8687e-02,  8.3463e-02, -1.5634e-02,\n",
            "         -1.6781e-01, -2.1426e-01,  1.8463e-01,  8.3891e-02,  5.9950e-02,\n",
            "         -2.0538e-01, -2.7832e-02,  4.7442e-02, -1.9782e-01, -1.7842e-01,\n",
            "          1.1362e-01,  1.4100e-01, -1.3794e-01,  1.1704e-01, -3.4108e-02],\n",
            "        [ 3.8388e-02, -1.7268e-01, -1.0235e-01, -1.2634e-01, -1.1883e-01,\n",
            "         -1.3463e-01, -1.7610e-01,  3.6543e-02, -1.7834e-01, -1.6471e-01,\n",
            "          2.0834e-01,  1.8400e-01, -8.8723e-02, -7.5378e-02,  1.7877e-01,\n",
            "         -5.7259e-02, -2.4522e-02, -1.1822e-02, -1.8196e-01,  1.9812e-01],\n",
            "        [-2.2011e-02,  2.1847e-01,  1.8410e-01,  9.7177e-02, -5.0634e-03,\n",
            "         -2.4731e-03,  5.1408e-03, -2.1733e-01, -5.3375e-02, -1.0346e-01,\n",
            "         -1.3303e-02,  2.7354e-02, -1.7523e-01,  1.6994e-01,  1.8259e-01,\n",
            "          1.3907e-01,  1.0041e-01,  3.5377e-02, -1.6114e-01,  9.0056e-02],\n",
            "        [ 7.9232e-02,  2.1614e-01, -2.1087e-01,  1.9407e-01,  1.7559e-01,\n",
            "          4.1470e-02,  7.4482e-02,  2.6737e-02, -1.7872e-02,  4.5040e-02,\n",
            "          1.2947e-01,  2.5483e-02, -2.0320e-02, -7.3942e-03, -1.7221e-01,\n",
            "         -1.0705e-01,  1.8203e-01,  1.3179e-02,  2.3468e-02, -1.9567e-01],\n",
            "        [ 1.6338e-01,  8.0209e-03, -2.9885e-02, -2.1884e-01,  1.3471e-01,\n",
            "         -2.8901e-02, -1.8757e-01,  8.9256e-03,  2.0940e-01,  9.0927e-02,\n",
            "         -8.2969e-02, -9.0893e-03,  1.0047e-01, -1.6897e-02, -1.3736e-01,\n",
            "          1.6801e-01, -1.9342e-01, -3.4822e-02,  1.0057e-01,  2.2273e-02],\n",
            "        [ 1.4611e-01,  1.4414e-01, -2.3093e-02,  8.1946e-02,  5.9792e-03,\n",
            "          6.7672e-02,  1.5254e-01,  1.6742e-01, -1.6896e-01,  1.1571e-01,\n",
            "         -1.8538e-01,  2.3316e-02, -1.6147e-01,  1.0230e-01, -1.7314e-01,\n",
            "         -1.8906e-01, -2.0286e-01, -2.1210e-02, -2.1799e-02, -3.7921e-02],\n",
            "        [ 1.9375e-01,  5.3921e-02, -1.4900e-01,  1.6709e-01, -1.6652e-01,\n",
            "          6.2363e-02, -4.1574e-02, -2.0565e-01, -1.3649e-01, -2.0600e-01,\n",
            "         -1.9032e-01, -8.8942e-02, -7.8061e-02,  1.6323e-01, -1.3174e-01,\n",
            "          5.8638e-02,  2.1117e-01,  1.6707e-01, -5.9492e-02, -2.0973e-01],\n",
            "        [-2.5644e-02, -1.0818e-02, -3.3051e-02,  3.7071e-02, -1.0809e-01,\n",
            "          2.0642e-01,  1.2396e-01, -2.1523e-01,  1.2172e-01, -1.4323e-01,\n",
            "          1.1334e-01,  4.6931e-02,  8.4553e-02,  2.0530e-01, -1.1833e-01,\n",
            "          1.9287e-01, -2.8398e-02,  7.1443e-03, -2.1055e-01,  1.0805e-01],\n",
            "        [-1.2258e-01, -6.8325e-02, -2.1929e-01, -1.4939e-01,  1.9226e-01,\n",
            "         -6.2922e-02, -7.6377e-02,  2.1955e-01, -4.5838e-02,  9.8011e-03,\n",
            "         -2.9401e-03, -9.5241e-02, -7.9775e-02, -1.8708e-01,  1.7828e-01,\n",
            "         -1.7552e-01, -1.0328e-01, -1.9697e-02, -1.7449e-01,  2.0408e-02]])\n",
            "tensor([[ 0.1366, -0.1346,  0.2128, -0.1776, -0.0682, -0.1541,  0.1725,  0.0839,\n",
            "         -0.1115, -0.1729, -0.1295, -0.0431, -0.1141,  0.1630, -0.0941, -0.1463,\n",
            "         -0.0690, -0.2184, -0.1086, -0.1220],\n",
            "        [ 0.0489,  0.1841,  0.0263, -0.1289,  0.0943, -0.0936, -0.1042, -0.0408,\n",
            "         -0.1147,  0.0261,  0.1834,  0.0325,  0.1369, -0.1934, -0.1741, -0.0278,\n",
            "          0.0023,  0.1420, -0.1957,  0.0950],\n",
            "        [-0.1910,  0.0385,  0.1501,  0.0329,  0.0739,  0.1439,  0.2245, -0.1381,\n",
            "          0.0736,  0.1868,  0.1293, -0.1998, -0.1534, -0.2173,  0.1007,  0.2246,\n",
            "         -0.2151,  0.1819, -0.2074, -0.0249],\n",
            "        [ 0.1814, -0.0350,  0.0249,  0.1630, -0.1877,  0.0563, -0.1091,  0.2050,\n",
            "         -0.1936,  0.0168,  0.1396, -0.1313, -0.1319,  0.0765, -0.1755,  0.1307,\n",
            "          0.0269, -0.2204,  0.0765, -0.1879],\n",
            "        [ 0.0276,  0.1371, -0.0388,  0.0834, -0.0161, -0.1677, -0.2149,  0.1835,\n",
            "          0.0843,  0.0590, -0.2057, -0.0285,  0.0485, -0.1975, -0.1786,  0.1132,\n",
            "          0.1409, -0.1385,  0.1169, -0.0344],\n",
            "        [ 0.0382, -0.1727, -0.1025, -0.1265, -0.1190, -0.1346, -0.1762,  0.0364,\n",
            "         -0.1784, -0.1648,  0.2080,  0.1837, -0.0888, -0.0755,  0.1786, -0.0574,\n",
            "         -0.0246, -0.0119, -0.1819,  0.1979],\n",
            "        [-0.0239,  0.2165,  0.1828,  0.0956, -0.0064, -0.0040,  0.0043, -0.2193,\n",
            "         -0.0563, -0.1027, -0.0152,  0.0268, -0.1785,  0.1690,  0.1808,  0.1382,\n",
            "          0.0988,  0.0339, -0.1620,  0.0902],\n",
            "        [ 0.0793,  0.2141, -0.2135,  0.1922,  0.1731,  0.0406,  0.0719,  0.0249,\n",
            "         -0.0182,  0.0418,  0.1276,  0.0236, -0.0215, -0.0089, -0.1731, -0.1101,\n",
            "          0.1809,  0.0103,  0.0230, -0.1983],\n",
            "        [ 0.1638,  0.0079, -0.0301, -0.2186,  0.1346, -0.0291, -0.1879,  0.0090,\n",
            "          0.2107,  0.0907, -0.0822, -0.0100,  0.1014, -0.0171, -0.1368,  0.1675,\n",
            "         -0.1929, -0.0346,  0.1006,  0.0220],\n",
            "        [ 0.1464,  0.1444, -0.0241,  0.0815,  0.0060,  0.0674,  0.1529,  0.1667,\n",
            "         -0.1700,  0.1161, -0.1847,  0.0231, -0.1618,  0.1018, -0.1734, -0.1889,\n",
            "         -0.2030, -0.0209, -0.0213, -0.0381],\n",
            "        [ 0.1937,  0.0540, -0.1488,  0.1674, -0.1667,  0.0626, -0.0416, -0.2055,\n",
            "         -0.1362, -0.2062, -0.1904, -0.0888, -0.0780,  0.1632, -0.1316,  0.0585,\n",
            "          0.2112,  0.1673, -0.0592, -0.2097],\n",
            "        [-0.0270, -0.0128, -0.0361,  0.0345, -0.1106,  0.2051,  0.1223, -0.2168,\n",
            "          0.1192, -0.1452,  0.1118,  0.0452,  0.0831,  0.2024, -0.1201,  0.1910,\n",
            "         -0.0294,  0.0062, -0.2124,  0.1063],\n",
            "        [-0.1225, -0.0683, -0.2193, -0.1493,  0.1925, -0.0628, -0.0763,  0.2198,\n",
            "         -0.0458,  0.0101, -0.0028, -0.0951, -0.0797, -0.1871,  0.1785, -0.1755,\n",
            "         -0.1032, -0.0194, -0.1744,  0.0205]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import Module\n",
        "# torch.optim实现带动量的梯度下降\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# 确定数据、确定优先需要设置的值\n",
        "lr = 0.1\n",
        "gamma = 0.9\n",
        "\n",
        "torch.manual_seed(420)\n",
        "X = torch.rand((500, 20), dtype=torch.float32) * 100\n",
        "y = torch.randint(low=0, high=3, size=(500, 1), dtype=torch.float32)\n",
        "input_ = X.shape[1] # 特征的数目\n",
        "output_ = len(y.unique()) # 分类的数目\n",
        "\n",
        "# 定义神经网络的架构\n",
        "class Model(nn.Module):\n",
        "  def __init__(self, in_features, out_features):\n",
        "    super().__init__()\n",
        "\n",
        "    self.linear1 = nn.Linear(in_features, 13, bias=True)\n",
        "    self.linear2 = nn.Linear(13, 8, bias=True)\n",
        "    self.output = nn.Linear(8, out_features, bias=True)\n",
        "  \n",
        "  def forward(self, X):\n",
        "    z1 = self.linear1(X)\n",
        "    sigma1 = torch.relu(z1)\n",
        "    z2 = self.linear2(sigma1)\n",
        "    sigma2 = torch.sigmoid(z2)\n",
        "    z3 = self.output(sigma2)\n",
        "    #sigma3 = F.softmax(z3)\n",
        "    return z3\n",
        "  \n",
        "# 实例化神经网络, 调用优化算法需要的参数\n",
        "torch.manual_seed(420)\n",
        "net = Model(in_features=input_, out_features=output_)\n",
        "net.parameters() # 一次性导出神经网络架构下全部的权重和截距\n",
        "\n",
        "# 定义损失函数\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 定义优化算法\n",
        "opt = optim.SGD(net.parameters(), # 要优化的参数是哪些\n",
        "         lr = lr, # 学习率\n",
        "         momentum = gamma) # 动量参数"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJde3xANNmsx",
        "outputId": "228c5ef4-19cd-4846-a211-86ada47be2e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<generator object Module.parameters at 0x7fecac82de50>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 接下来开始进行一轮梯度下降\n",
        "zhat = net.forward(X) #前向传播\n",
        "loss = criterion(zhat, y.reshape(500).long()) # 损失函数值\n",
        "loss.backward() # 反向传播\n",
        "opt.step() # 更新权重w,从这一瞬间开始，坐标点就发生了变化，所有的梯度必须重新计算\n",
        "opt.zero_grad() # 清除原来储存好的，基于上一个坐标点计算的梯度，为下一次计算梯度腾出空间\n",
        "\n",
        "print(loss)\n",
        "print(net.linear1.weight.data[0][:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wb10vjT_dj90",
        "outputId": "e1ad4acf-32e2-493c-8a61-c3155c74b683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.0851, grad_fn=<NllLossBackward0>)\n",
            "tensor([ 0.1380, -0.1342,  0.2146, -0.1774, -0.0676, -0.1538,  0.1740,  0.0851,\n",
            "        -0.1111, -0.1727])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TensorDataset与DataLoader\n",
        "# TensorDataset -将特征与标签合并到同一个对象中\n",
        "# LoadData -帮助我们进行小批量的分割\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "a = torch.randn(500, 2, 3)\n",
        "b = torch.randn(500, 3, 4, 5)\n",
        "c = torch.randn(500, 1)\n",
        "\n",
        "# 被合并的对象第一维度上的值相等\n",
        "for i in TensorDataset(b, c):\n",
        "  print(i)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lK7vzaffOpm",
        "outputId": "d49d0f12-de24-4f36-b9b3-fceff0ca87ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[[ 1.4324, -1.5099,  0.9286, -0.2416,  0.5485],\n",
            "         [ 2.4199, -0.0434, -0.0223,  0.4258, -2.3547],\n",
            "         [ 0.5817, -0.1884, -1.4250, -0.5167, -0.6050],\n",
            "         [-0.0143,  0.1180,  0.3792,  0.6414,  0.1110]],\n",
            "\n",
            "        [[ 0.1669,  0.2931,  3.3509,  1.6537,  1.2335],\n",
            "         [-0.8471, -0.1266, -0.1298,  0.1220, -0.4691],\n",
            "         [-0.0292,  0.5957,  0.4642, -0.6832,  1.3422],\n",
            "         [-0.3683,  1.6137,  0.0393,  2.1853,  1.5552]],\n",
            "\n",
            "        [[ 0.9770,  0.2914,  1.3271, -0.2651, -0.5037],\n",
            "         [ 2.1154, -0.6312,  0.1289, -1.2272, -0.0769],\n",
            "         [ 0.3284, -0.4702, -0.1327, -1.3445,  1.3452],\n",
            "         [-0.3232,  0.7399,  0.2545,  0.0505,  0.9385]]]), tensor([-0.7568]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用划分小批量的功能DataLoader\n",
        "from torch.utils.data import DataLoader\n",
        "data = TensorDataset(b, c)\n",
        "for i in DataLoader(data):\n",
        "  print(i)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CLMzn8MsH0f",
        "outputId": "7811e218-3bd4-4bc4-8974-a33187b988f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[[[ 1.4324, -1.5099,  0.9286, -0.2416,  0.5485],\n",
            "          [ 2.4199, -0.0434, -0.0223,  0.4258, -2.3547],\n",
            "          [ 0.5817, -0.1884, -1.4250, -0.5167, -0.6050],\n",
            "          [-0.0143,  0.1180,  0.3792,  0.6414,  0.1110]],\n",
            "\n",
            "         [[ 0.1669,  0.2931,  3.3509,  1.6537,  1.2335],\n",
            "          [-0.8471, -0.1266, -0.1298,  0.1220, -0.4691],\n",
            "          [-0.0292,  0.5957,  0.4642, -0.6832,  1.3422],\n",
            "          [-0.3683,  1.6137,  0.0393,  2.1853,  1.5552]],\n",
            "\n",
            "         [[ 0.9770,  0.2914,  1.3271, -0.2651, -0.5037],\n",
            "          [ 2.1154, -0.6312,  0.1289, -1.2272, -0.0769],\n",
            "          [ 0.3284, -0.4702, -0.1327, -1.3445,  1.3452],\n",
            "          [-0.3232,  0.7399,  0.2545,  0.0505,  0.9385]]]]), tensor([[-0.7568]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DataLoad()的参数\n",
        "bs = 120\n",
        "dataset = DataLoader(data,\n",
        "           batch_size=bs,\n",
        "           shuffle=True,\n",
        "           drop_last=True)\n",
        "\n",
        "for i in dataset:\n",
        "  print(i[0].shape)\n",
        "\n",
        "print(len(dataset)) # 一共有多少个batch\n",
        "print(len(dataset.dataset)) # 展示里面全部的数据\n",
        "print(dataset.dataset[0]) # 单个样本\n",
        "print(dataset.dataset[0][0]) # 单个样本的特征\n",
        "print(dataset.dataset[0][1]) # 单个样本的标签\n",
        "print(dataset.batch_size) # 查看现有的batch_size\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWsXD0SKvuDR",
        "outputId": "ecea48b4-0685-4d2d-aa34-410fac49494d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([120, 3, 4, 5])\n",
            "torch.Size([120, 3, 4, 5])\n",
            "torch.Size([120, 3, 4, 5])\n",
            "torch.Size([120, 3, 4, 5])\n",
            "4\n",
            "500\n",
            "(tensor([[[ 1.4324, -1.5099,  0.9286, -0.2416,  0.5485],\n",
            "         [ 2.4199, -0.0434, -0.0223,  0.4258, -2.3547],\n",
            "         [ 0.5817, -0.1884, -1.4250, -0.5167, -0.6050],\n",
            "         [-0.0143,  0.1180,  0.3792,  0.6414,  0.1110]],\n",
            "\n",
            "        [[ 0.1669,  0.2931,  3.3509,  1.6537,  1.2335],\n",
            "         [-0.8471, -0.1266, -0.1298,  0.1220, -0.4691],\n",
            "         [-0.0292,  0.5957,  0.4642, -0.6832,  1.3422],\n",
            "         [-0.3683,  1.6137,  0.0393,  2.1853,  1.5552]],\n",
            "\n",
            "        [[ 0.9770,  0.2914,  1.3271, -0.2651, -0.5037],\n",
            "         [ 2.1154, -0.6312,  0.1289, -1.2272, -0.0769],\n",
            "         [ 0.3284, -0.4702, -0.1327, -1.3445,  1.3452],\n",
            "         [-0.3232,  0.7399,  0.2545,  0.0505,  0.9385]]]), tensor([-0.7568]))\n",
            "tensor([[[ 1.4324, -1.5099,  0.9286, -0.2416,  0.5485],\n",
            "         [ 2.4199, -0.0434, -0.0223,  0.4258, -2.3547],\n",
            "         [ 0.5817, -0.1884, -1.4250, -0.5167, -0.6050],\n",
            "         [-0.0143,  0.1180,  0.3792,  0.6414,  0.1110]],\n",
            "\n",
            "        [[ 0.1669,  0.2931,  3.3509,  1.6537,  1.2335],\n",
            "         [-0.8471, -0.1266, -0.1298,  0.1220, -0.4691],\n",
            "         [-0.0292,  0.5957,  0.4642, -0.6832,  1.3422],\n",
            "         [-0.3683,  1.6137,  0.0393,  2.1853,  1.5552]],\n",
            "\n",
            "        [[ 0.9770,  0.2914,  1.3271, -0.2651, -0.5037],\n",
            "         [ 2.1154, -0.6312,  0.1289, -1.2272, -0.0769],\n",
            "         [ 0.3284, -0.4702, -0.1327, -1.3445,  1.3452],\n",
            "         [-0.3232,  0.7399,  0.2545,  0.0505,  0.9385]]])\n",
            "tensor([-0.7568])\n",
            "120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 对于小批量随机梯度下降而言，我们一般这样使用TensorDataset与DataLoader:\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "torch.manual_seed(420)\n",
        "X = torch.rand((50000, 20), dtype=torch.float32) * 100 \n",
        "y = torch.randint(low=0, high=3, size=(50000, 1), dtype=torch.float32)\n",
        "\n",
        "epochs = 4 # 请让神经网络学习4次数据\n",
        "bs = 4000\n",
        "\n",
        "data = TensorDataset(X, y)\n",
        "batchdata = DataLoader(data,\n",
        "            batch_size=bs,\n",
        "            shuffle=True)\n",
        "\n",
        "print(len(batchdata)) # 查看具体被分了多少个batch 13\n",
        "\n",
        "# 可以使用.datasets查看数据集相关的属性\n",
        "print(len(batchdata.dataset)) # 总共有多少数据 50000\n",
        "print(batchdata.dataset[0]) # 查看其中一个样本\n",
        "print(batchdata.dataset[0][0]) # 一个样本的特征张量\n",
        "print(batchdata.dataset[0][0]) # 一个样本的标签\n",
        "print(batchdata.batch_size) # 查看现在的batch_size是多少 4000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EjTGoZe3wDr",
        "outputId": "c3942fe3-c9e8-40f1-bbec-98c1f14adf19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "13\n",
            "50000\n",
            "(tensor([80.5354, 19.9040, 97.5853, 10.2817, 34.7460, 15.5433, 88.5615, 68.7572,\n",
            "        25.0620, 11.3297, 21.0499, 40.3541, 24.4790, 86.4358, 28.9626, 17.2895,\n",
            "        34.5751,  1.1728, 25.7179, 22.7224]), tensor([2.]))\n",
            "tensor([80.5354, 19.9040, 97.5853, 10.2817, 34.7460, 15.5433, 88.5615, 68.7572,\n",
            "        25.0620, 11.3297, 21.0499, 40.3541, 24.4790, 86.4358, 28.9626, 17.2895,\n",
            "        34.5751,  1.1728, 25.7179, 22.7224])\n",
            "tensor([80.5354, 19.9040, 97.5853, 10.2817, 34.7460, 15.5433, 88.5615, 68.7572,\n",
            "        25.0620, 11.3297, 21.0499, 40.3541, 24.4790, 86.4358, 28.9626, 17.2895,\n",
            "        34.5751,  1.1728, 25.7179, 22.7224])\n",
            "4000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 在MINST-FASHION上实现神将网络的实现流程\n",
        "import torch\n",
        "import torch.nn\n",
        "from torch import optim\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 确定数据\n",
        "lr = 0.15\n",
        "gamma = 0\n",
        "epochs = 10\n",
        "bs = 128\n",
        "\n",
        "# 导入数据，分割小批量\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# 初次运行时会下载，需要等待较长的时间\n",
        "mnist = torchvision.datasets.FashionMNIST(\n",
        "    root = 'C:\\学习资料文件夹\\深度之眼\\菜菜Pytorch深度学习\\PyTorch课件\\WEEK 3、4\\Datasets\\FashionMNIST\\FashionMNIST',\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = transforms.ToTensor()\n",
        "    )\n",
        "\n",
        "print(mnist)\n",
        "print(len(mnist)) # 60000\n",
        "print(mnist.data)\n",
        "print(mnist.data.shape) # torch.Size([60000, 28, 28])\n",
        "print(mnist.targets.unique())\n",
        "print(mnist.classes) # 标签分类信息\n",
        "\n",
        "# 查看图像的模样\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy\n",
        "\n",
        "print(mnist[0][0].shape) # torch.Size([1, 28, 28])\n",
        "print(mnist[0][0])\n",
        "print(mnist[0][0].view(28, 28).numpy())\n",
        "plt.imshow(mnist[0][0].view(28, 28).numpy())\n",
        "plt.imshow(mnist[1][0].view(28, 28).numpy())"
      ],
      "metadata": {
        "id": "g3_iYf3QAgkQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "22513542-589f-4f2d-f15f-2bc24faf46ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset FashionMNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: C:\\学习资料文件夹\\深度之眼\\菜菜Pytorch深度学习\\PyTorch课件\\WEEK 3、4\\Datasets\\FashionMNIST\\FashionMNIST\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: ToTensor()\n",
            "60000\n",
            "tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0]]], dtype=torch.uint8)\n",
            "torch.Size([60000, 28, 28])\n",
            "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
            "['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
            "torch.Size([1, 28, 28])\n",
            "tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n",
            "          0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0039, 0.0039, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n",
            "          0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n",
            "          0.0157, 0.0000, 0.0000, 0.0118],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n",
            "          0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0471, 0.0392, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n",
            "          0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n",
            "          0.3020, 0.5098, 0.2824, 0.0588],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n",
            "          0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n",
            "          0.5529, 0.3451, 0.6745, 0.2588],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n",
            "          0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n",
            "          0.4824, 0.7686, 0.8980, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n",
            "          0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n",
            "          0.8745, 0.9608, 0.6784, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n",
            "          0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n",
            "          0.8627, 0.9529, 0.7922, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n",
            "          0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n",
            "          0.8863, 0.7725, 0.8196, 0.2039],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n",
            "          0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n",
            "          0.9608, 0.4667, 0.6549, 0.2196],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n",
            "          0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n",
            "          0.8510, 0.8196, 0.3608, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n",
            "          0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n",
            "          0.8549, 1.0000, 0.3020, 0.0000],\n",
            "         [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n",
            "          0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n",
            "          0.8784, 0.9569, 0.6235, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n",
            "          0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n",
            "          0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n",
            "          0.9137, 0.9333, 0.8431, 0.0000],\n",
            "         [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n",
            "          0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n",
            "          0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n",
            "          0.8627, 0.9098, 0.9647, 0.0000],\n",
            "         [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n",
            "          0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n",
            "          0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n",
            "          0.8706, 0.8941, 0.8824, 0.0000],\n",
            "         [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n",
            "          0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n",
            "          0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n",
            "          0.8745, 0.8784, 0.8980, 0.1137],\n",
            "         [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n",
            "          0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n",
            "          0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n",
            "          0.8627, 0.8667, 0.9020, 0.2627],\n",
            "         [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n",
            "          0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n",
            "          0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n",
            "          0.7098, 0.8039, 0.8078, 0.4510],\n",
            "         [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n",
            "          0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n",
            "          0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n",
            "          0.6549, 0.6941, 0.8235, 0.3608],\n",
            "         [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n",
            "          0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n",
            "          0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n",
            "          0.7529, 0.8471, 0.6667, 0.0000],\n",
            "         [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n",
            "          0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n",
            "          0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n",
            "          0.3882, 0.2275, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n",
            "          0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
            "          0.0000, 0.0000, 0.0000, 0.0000]]])\n",
            "[[0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.00392157 0.         0.         0.05098039 0.28627452 0.\n",
            "  0.         0.00392157 0.01568628 0.         0.         0.\n",
            "  0.         0.00392157 0.00392157 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.01176471 0.         0.14117648 0.53333336 0.49803922 0.24313726\n",
            "  0.21176471 0.         0.         0.         0.00392157 0.01176471\n",
            "  0.01568628 0.         0.         0.01176471]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.02352941 0.         0.4        0.8        0.6901961  0.5254902\n",
            "  0.5647059  0.48235294 0.09019608 0.         0.         0.\n",
            "  0.         0.04705882 0.03921569 0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.60784316 0.9254902  0.8117647  0.69803923\n",
            "  0.41960785 0.6117647  0.6313726  0.42745098 0.2509804  0.09019608\n",
            "  0.3019608  0.50980395 0.28235295 0.05882353]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.00392157\n",
            "  0.         0.27058825 0.8117647  0.8745098  0.85490197 0.84705883\n",
            "  0.84705883 0.6392157  0.49803922 0.4745098  0.47843137 0.57254905\n",
            "  0.5529412  0.34509805 0.6745098  0.25882354]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.00392157 0.00392157 0.00392157\n",
            "  0.         0.78431374 0.9098039  0.9098039  0.9137255  0.8980392\n",
            "  0.8745098  0.8745098  0.84313726 0.8352941  0.6431373  0.49803922\n",
            "  0.48235294 0.76862746 0.8980392  0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.7176471  0.88235295 0.84705883 0.8745098  0.89411765\n",
            "  0.92156863 0.8901961  0.8784314  0.87058824 0.8784314  0.8666667\n",
            "  0.8745098  0.9607843  0.6784314  0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.75686276 0.89411765 0.85490197 0.8352941  0.7764706\n",
            "  0.7058824  0.83137256 0.8235294  0.827451   0.8352941  0.8745098\n",
            "  0.8627451  0.9529412  0.7921569  0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.00392157 0.01176471 0.\n",
            "  0.04705882 0.85882354 0.8627451  0.83137256 0.85490197 0.7529412\n",
            "  0.6627451  0.8901961  0.8156863  0.85490197 0.8784314  0.83137256\n",
            "  0.8862745  0.77254903 0.81960785 0.20392157]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.02352941 0.\n",
            "  0.3882353  0.95686275 0.87058824 0.8627451  0.85490197 0.79607844\n",
            "  0.7764706  0.8666667  0.84313726 0.8352941  0.87058824 0.8627451\n",
            "  0.9607843  0.46666667 0.654902   0.21960784]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.01568628 0.         0.\n",
            "  0.21568628 0.9254902  0.89411765 0.9019608  0.89411765 0.9411765\n",
            "  0.9098039  0.8352941  0.85490197 0.8745098  0.91764706 0.8509804\n",
            "  0.8509804  0.81960785 0.36078432 0.        ]\n",
            " [0.         0.         0.00392157 0.01568628 0.02352941 0.02745098\n",
            "  0.00784314 0.         0.         0.         0.         0.\n",
            "  0.92941177 0.8862745  0.8509804  0.8745098  0.87058824 0.85882354\n",
            "  0.87058824 0.8666667  0.84705883 0.8745098  0.8980392  0.84313726\n",
            "  0.85490197 1.         0.3019608  0.        ]\n",
            " [0.         0.01176471 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.24313726 0.5686275  0.8\n",
            "  0.89411765 0.8117647  0.8352941  0.8666667  0.85490197 0.8156863\n",
            "  0.827451   0.85490197 0.8784314  0.8745098  0.85882354 0.84313726\n",
            "  0.8784314  0.95686275 0.62352943 0.        ]\n",
            " [0.         0.         0.         0.         0.07058824 0.17254902\n",
            "  0.32156864 0.41960785 0.7411765  0.89411765 0.8627451  0.87058824\n",
            "  0.8509804  0.8862745  0.78431374 0.8039216  0.827451   0.9019608\n",
            "  0.8784314  0.91764706 0.6901961  0.7372549  0.98039216 0.972549\n",
            "  0.9137255  0.93333334 0.84313726 0.        ]\n",
            " [0.         0.22352941 0.73333335 0.8156863  0.8784314  0.8666667\n",
            "  0.8784314  0.8156863  0.8        0.8392157  0.8156863  0.81960785\n",
            "  0.78431374 0.62352943 0.9607843  0.75686276 0.80784315 0.8745098\n",
            "  1.         1.         0.8666667  0.91764706 0.8666667  0.827451\n",
            "  0.8627451  0.9098039  0.9647059  0.        ]\n",
            " [0.01176471 0.7921569  0.89411765 0.8784314  0.8666667  0.827451\n",
            "  0.827451   0.8392157  0.8039216  0.8039216  0.8039216  0.8627451\n",
            "  0.9411765  0.3137255  0.5882353  1.         0.8980392  0.8666667\n",
            "  0.7372549  0.6039216  0.7490196  0.8235294  0.8        0.81960785\n",
            "  0.87058824 0.89411765 0.88235295 0.        ]\n",
            " [0.38431373 0.9137255  0.7764706  0.8235294  0.87058824 0.8980392\n",
            "  0.8980392  0.91764706 0.9764706  0.8627451  0.7607843  0.84313726\n",
            "  0.8509804  0.94509804 0.25490198 0.28627452 0.41568628 0.45882353\n",
            "  0.65882355 0.85882354 0.8666667  0.84313726 0.8509804  0.8745098\n",
            "  0.8745098  0.8784314  0.8980392  0.11372549]\n",
            " [0.29411766 0.8        0.83137256 0.8        0.75686276 0.8039216\n",
            "  0.827451   0.88235295 0.84705883 0.7254902  0.77254903 0.80784315\n",
            "  0.7764706  0.8352941  0.9411765  0.7647059  0.8901961  0.9607843\n",
            "  0.9372549  0.8745098  0.85490197 0.83137256 0.81960785 0.87058824\n",
            "  0.8627451  0.8666667  0.9019608  0.2627451 ]\n",
            " [0.1882353  0.79607844 0.7176471  0.7607843  0.8352941  0.77254903\n",
            "  0.7254902  0.74509805 0.7607843  0.7529412  0.7921569  0.8392157\n",
            "  0.85882354 0.8666667  0.8627451  0.9254902  0.88235295 0.84705883\n",
            "  0.78039217 0.80784315 0.7294118  0.70980394 0.69411767 0.6745098\n",
            "  0.70980394 0.8039216  0.80784315 0.4509804 ]\n",
            " [0.         0.47843137 0.85882354 0.75686276 0.7019608  0.67058825\n",
            "  0.7176471  0.76862746 0.8        0.8235294  0.8352941  0.8117647\n",
            "  0.827451   0.8235294  0.78431374 0.76862746 0.7607843  0.7490196\n",
            "  0.7647059  0.7490196  0.7764706  0.7529412  0.6901961  0.6117647\n",
            "  0.654902   0.69411767 0.8235294  0.36078432]\n",
            " [0.         0.         0.2901961  0.7411765  0.83137256 0.7490196\n",
            "  0.6862745  0.6745098  0.6862745  0.70980394 0.7254902  0.7372549\n",
            "  0.7411765  0.7372549  0.75686276 0.7764706  0.8        0.81960785\n",
            "  0.8235294  0.8235294  0.827451   0.7372549  0.7372549  0.7607843\n",
            "  0.7529412  0.84705883 0.6666667  0.        ]\n",
            " [0.00784314 0.         0.         0.         0.25882354 0.78431374\n",
            "  0.87058824 0.92941177 0.9372549  0.9490196  0.9647059  0.9529412\n",
            "  0.95686275 0.8666667  0.8627451  0.75686276 0.7490196  0.7019608\n",
            "  0.7137255  0.7137255  0.70980394 0.6901961  0.6509804  0.65882355\n",
            "  0.3882353  0.22745098 0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.15686275 0.23921569 0.17254902 0.28235295 0.16078432\n",
            "  0.13725491 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.        ]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f1d2daec710>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 确定数据，确定超参数\n",
        "lr = 0.15\n",
        "gamma = 0\n",
        "epochs = 10\n",
        "bs = 128\n",
        "\n",
        "mnist = torchvision.datasets.FashionMNIST(\n",
        "    root = 'C:\\学习资料文件夹\\深度之眼\\菜菜Pytorch深度学习\\PyTorch课件\\WEEK 3、4\\Datasets\\FashionMNIST\\FashionMNIST',\n",
        "    download = True,\n",
        "    train = True,\n",
        "    transform = transforms.ToTensor()\n",
        "    )\n",
        "\n",
        "batchdata = DataLoader(mnist, batch_size=bs, shuffle=True)\n",
        "print(batchdata)\n",
        "for X,y in batchdata:\n",
        "  print(X.shape)\n",
        "  print(y.shape)\n",
        "  break\n",
        "\n",
        "input_ = mnist.data[0].numel() # 请问这个张量中总共有多少个元素\n",
        "output_ = len(mnist.targets.unique())\n",
        "print(input_) # 784\n",
        "print(output_) # 10\n",
        "\n",
        "# 定义神经网络的架构\n",
        "class Model(nn.Module):\n",
        "  def __init__(self, in_features, out_features):\n",
        "    super().__init__\n",
        "    self.linear1 = nn.Linear(in_features, 1280, bias=False)\n",
        "    self.output = nn.Linear(1280, out_features, bias=False)\n",
        "  \n",
        "  def forward(self, X):\n",
        "    X = X.view(-1, 28 * 28)\n",
        "    z1 = self.linear1(X)\n",
        "    sigma1 = torch.relu(z1)\n",
        "    z2 = self.output(sigma1)\n",
        "    sigma2 = F.log_softmax(z2, dim=1)\n",
        "    return sigma2\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hyare3Mdq7lo",
        "outputId": "eeba839b-5ccb-4c9d-9ed3-da7db2da005a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7f1d2d9a3e10>\n",
            "torch.Size([128, 1, 28, 28])\n",
            "torch.Size([128])\n",
            "784\n",
            "10\n"
          ]
        }
      ]
    }
  ]
}