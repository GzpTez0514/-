{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch深度学习08.Pytorch实现反向传播",
      "provenance": [],
      "authorship_tag": "ABX9TyM0Y0PA3SYCb6dIxrjaBdGq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GzpTez0514/-/blob/main/Pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A008_Pytorch%E5%AE%9E%E7%8E%B0%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yURSUPbBN3j1",
        "outputId": "a4c605ae-3c79-471a-cac5-cf47e574ee49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor(2.),)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "X = torch.tensor(1., requires_grad=True) # requires_grad 表示允许对X进行梯度计算\n",
        "y = X ** 2\n",
        "\n",
        "grad = torch.autograd.grad(y, X) # 这里返回的是在函数y = X ** 2上，X = 1时的导数值\n",
        "print(grad)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 对于单层神经网络，autograd.grad会非常有效。但深层神经网络就不太适合使用grad函数了\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "torch.manual_seed(420)\n",
        "X = torch.rand((500, 20), dtype=torch.float32) * 100\n",
        "y = torch.randint(low=0, high=3, size=(500, 1), dtype=torch.float32)\n",
        "\n",
        "# 定义神经网络的架构\n",
        "class Model(nn.Module):\n",
        "  def __init__(self, in_features=10, out_features=2):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(in_features, 13, bias=True)\n",
        "    self.linear2 = nn.Linear(13, 8, bias=True)\n",
        "    self.output = nn.Linear(8, out_features, bias=True)\n",
        "    \n",
        "  def forward(self, X):\n",
        "    z1 = self.linear1(X)\n",
        "    sigma1 = torch.relu(z1)\n",
        "    z2 = self.linear2(sigma1)\n",
        "    sigma2 = torch.sigmoid(z2)\n",
        "    z3 = self.output(sigma2)\n",
        "  # sigma3 = F.softmax(z3, dim=1)\n",
        "    return z3\n",
        "\n",
        "input_ = X.shape[1] # 特征的数目\n",
        "output_ = len(y.unique()) # 分类的数目\n",
        "\n",
        "# 实例化神经网络类\n",
        "torch.manual_seed(420)\n",
        "net = Model(in_features=input_, out_features=output_)\n",
        "# 正向传播\n",
        "zhat = net.forward(X)\n",
        "# 定义损失函数\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# 对于打包好的CrossEntropyLoss而言，只需要输入zhat\n",
        "loss = criterion(zhat, y.reshape(500).long())\n",
        "print(loss)\n",
        "print(net.linear1.weight.grad) # 不会返回任何值\n",
        "# 反向传播，backward是任意损失函数类都可以调用的方法，对任意损失函数，backward都会求解其中全部w的梯度\n",
        "loss.backward()\n",
        "print(net.linear1.weight.grad) # 返回响应的梯度\n",
        "\n",
        "# 与可以重复进行的正向传播不同，一次正向传播后，反向传播只能进行一次\n",
        "# 如果希望可以重复进行反向传播，可以在第一次进行反向传播的时候加上参数retain_graph\n",
        "loss.backward(retain_graph=True)\n",
        "loss.backward()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMNl-ATpPrjx",
        "outputId": "53b44b06-031d-4388-c21f-a93111b69a2d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.1057, grad_fn=<NllLossBackward0>)\n",
            "None\n",
            "tensor([[ 3.3727e-04,  8.3354e-05,  4.0867e-04,  4.3058e-05,  1.4551e-04,\n",
            "          6.5092e-05,  3.7088e-04,  2.8794e-04,  1.0495e-04,  4.7446e-05,\n",
            "          8.8153e-05,  1.6899e-04,  1.0251e-04,  3.6197e-04,  1.2129e-04,\n",
            "          7.2405e-05,  1.4479e-04,  4.9114e-06,  1.0770e-04,  9.5156e-05],\n",
            "        [ 8.2042e-03,  2.1974e-02,  2.1073e-02,  1.3896e-02,  2.2161e-02,\n",
            "          1.5936e-02,  1.6537e-02,  2.0259e-02,  1.9655e-02,  1.4728e-02,\n",
            "          1.9212e-02,  2.0086e-02,  1.8295e-02,  8.4132e-03,  1.8036e-02,\n",
            "          1.9979e-02,  2.0966e-02,  2.4730e-02,  9.3876e-03,  1.7475e-02],\n",
            "        [ 9.1603e-03,  2.4275e-02,  2.3446e-02,  2.0096e-02,  2.5360e-02,\n",
            "          1.7406e-02,  3.2555e-02,  2.2461e-02,  3.6793e-03,  2.7445e-02,\n",
            "          2.1181e-02,  2.7724e-02,  1.7115e-02,  1.6943e-02,  1.7249e-02,\n",
            "          3.3173e-02,  1.5115e-02,  3.0874e-02,  1.8391e-02,  2.4201e-02],\n",
            "        [-2.8595e-04,  1.2968e-03,  1.3652e-03, -5.6692e-05, -1.7480e-03,\n",
            "         -2.6459e-03,  3.7307e-04, -2.7976e-03,  1.7848e-03, -9.9289e-04,\n",
            "         -3.3944e-04,  2.2783e-04,  1.2076e-03, -3.2906e-04,  5.4641e-04,\n",
            "         -2.8959e-03, -2.7890e-03, -3.0774e-03, -3.8981e-03, -6.0863e-03],\n",
            "        [ 1.5692e-03,  1.3161e-03, -9.0900e-04, -1.0158e-03, -4.9426e-03,\n",
            "          1.3061e-03, -6.8428e-03, -1.0955e-02,  4.2490e-03, -9.7841e-03,\n",
            "         -3.5231e-03, -6.9716e-03,  1.0742e-02,  2.9732e-03, -2.2283e-03,\n",
            "         -4.3101e-03, -1.2823e-03, -6.0040e-03, -1.5857e-03, -3.2208e-03],\n",
            "        [-2.3456e-03,  3.0270e-04, -9.7571e-04, -1.7812e-03, -1.9554e-03,\n",
            "          4.7728e-04, -7.5906e-04, -9.3554e-04, -8.6828e-04, -4.9501e-04,\n",
            "         -3.2291e-03, -2.6119e-03, -9.4872e-04, -1.3638e-03, -1.7115e-03,\n",
            "         -9.1220e-04, -1.0843e-03, -8.5492e-04,  1.5598e-04, -2.5193e-03],\n",
            "        [-1.8459e-02, -2.0009e-02, -1.3430e-02, -1.5675e-02, -1.3660e-02,\n",
            "         -1.4804e-02, -8.0083e-03, -2.0027e-02, -2.9173e-02,  7.5748e-03,\n",
            "         -1.8572e-02, -5.4350e-03, -3.2866e-02, -9.2504e-03, -1.8047e-02,\n",
            "         -9.1732e-03, -1.6036e-02, -1.4584e-02, -8.9602e-03,  1.7230e-03],\n",
            "        [ 9.5595e-04, -2.0620e-02, -2.6421e-02, -1.9139e-02, -2.5206e-02,\n",
            "         -9.0114e-03, -2.5945e-02, -1.8858e-02, -3.5860e-03, -3.2187e-02,\n",
            "         -1.8400e-02, -1.8522e-02, -1.1765e-02, -1.5216e-02, -8.4282e-03,\n",
            "         -3.0337e-02, -1.1364e-02, -2.9159e-02, -5.1049e-03, -2.6377e-02],\n",
            "        [ 3.7390e-03, -1.3553e-03, -2.2382e-03,  2.2693e-03, -1.4464e-03,\n",
            "         -2.4332e-03, -3.7881e-03,  7.3999e-04,  1.2527e-02, -1.9007e-03,\n",
            "          7.8561e-03, -9.5000e-03,  9.1147e-03, -2.2882e-03,  5.5781e-03,\n",
            "         -5.0263e-03,  4.8214e-03,  2.0531e-03,  4.1112e-04, -2.4565e-03],\n",
            "        [ 2.4338e-03,  2.9046e-03, -1.0326e-02, -4.4877e-03, -8.1686e-05,\n",
            "         -3.0232e-03,  3.2874e-03, -7.5255e-03, -1.0000e-02,  3.9967e-03,\n",
            "          6.3736e-03, -2.3521e-03, -3.5956e-03, -4.8027e-03, -2.7523e-03,\n",
            "          1.6497e-03, -1.3330e-03,  3.1406e-03,  4.9480e-03, -1.5097e-03],\n",
            "        [-7.3584e-04,  9.5657e-04,  1.6132e-03,  2.9669e-03, -1.5606e-03,\n",
            "          2.1566e-03, -5.9970e-04,  1.0974e-03,  2.8536e-03, -1.5761e-03,\n",
            "         -1.1973e-03,  1.4157e-03,  1.0819e-03, -1.6395e-04,  1.4259e-03,\n",
            "         -1.5993e-03,  4.9925e-04,  2.4320e-03,  2.8718e-03,  7.4142e-04],\n",
            "        [-1.3084e-02, -1.9413e-02, -3.0946e-02, -2.5855e-02, -2.5361e-02,\n",
            "         -1.3386e-02, -1.6360e-02, -1.5787e-02, -2.4764e-02, -2.0017e-02,\n",
            "         -1.5216e-02, -1.7701e-02, -1.4559e-02, -2.9177e-02, -1.7573e-02,\n",
            "         -1.8530e-02, -1.0069e-02, -9.5260e-03, -1.8987e-02, -1.7839e-02],\n",
            "        [ 4.8107e-04,  7.2480e-04,  9.8633e-05,  8.6471e-04,  1.9964e-03,\n",
            "          1.6202e-03,  1.1736e-03,  2.0290e-03,  3.3147e-04,  3.0171e-03,\n",
            "          1.0456e-03,  1.1400e-03,  5.0297e-04,  3.3159e-04,  1.9707e-03,\n",
            "          3.4884e-04,  8.4107e-04,  2.6777e-03,  8.6282e-04,  5.8412e-04]])\n"
          ]
        }
      ]
    }
  ]
}